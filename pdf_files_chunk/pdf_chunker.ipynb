{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f30db39",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9809c8b-f1ee-4932-9bf2-e1ba38eb7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install PyPDF2 pytesseract \n",
    "# ! pip install pymupdf\n",
    "# ! pip install langchain\n",
    "# ! pip install pathlib\n",
    "# ! pip install sentence-transformers\n",
    "# ! pip install glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85c96b5-6b47-4e89-b36f-fec726d61bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "import re\n",
    "import glob\n",
    "# from pathlib import Path\n",
    "\n",
    "from langchain.text_splitter import TextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f00e909d",
   "metadata": {},
   "source": [
    "# Defining functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71725e12-5179-48ab-8ef2-abffbe3d5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "    # Convert PDF pages to images using pdf2image\n",
    "    images = convert_from_path(pdf_path, poppler_path = r\"C:\\Users\\User\\Desktop\\pdf_parser\\venv_pdf\\Release-23.01.0-0\\poppler-23.01.0\\Library\\bin\")\n",
    "\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\User\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "    \n",
    "    # Iterate over each page\n",
    "    for page_num, image in enumerate(images):\n",
    "        # Perform OCR on the image using pytesseract\n",
    "        page_text = pytesseract.image_to_string(image, lang='eng')\n",
    "\n",
    "        # Identify and exclude text from diagrams\n",
    "        page_text = remove_diagram_text(page_text)\n",
    "\n",
    "        # Append the extracted text to the overall text\n",
    "        text += page_text\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_diagram_text(page_text):\n",
    "    # Add your custom logic to identify and remove text from diagrams\n",
    "    # You can use regex, string operations, or other techniques to identify and exclude text from diagrams\n",
    "\n",
    "    # For example, you can define a list of keywords or patterns commonly found in diagram text\n",
    "    diagram_keywords = ['diagram', 'chart', 'figure', 'graph']\n",
    "\n",
    "    # Split the page text into lines\n",
    "    lines = page_text.split('\\n')\n",
    "\n",
    "    # Iterate over each line and exclude lines containing diagram keywords\n",
    "    filtered_lines = [line for line in lines if not any(keyword in line.lower() for keyword in diagram_keywords)]\n",
    "\n",
    "    # Join the filtered lines back into a single string\n",
    "    filtered_text = '\\n'.join(filtered_lines)\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "# def load_docs(filename):\n",
    "#     document = ''\n",
    "#     loader = PyPDF2.PdfReader(filename)\n",
    "\n",
    "#     # finding max pages\n",
    "#     doc_page = len(loader.pages)\n",
    "\n",
    "#     #iterate through the pages and combine\n",
    "#     for i in range(doc_page):\n",
    "#       page = loader.pages[i]\n",
    "#       document += page.extract_text() + \"\"\n",
    "#     return document\n",
    "\n",
    "def text_chunker(file) :\n",
    "  # initiate TextSplitter class and input chunk size and overlap\n",
    "  text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "      model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "      tokens_per_chunk = 200,\n",
    "      chunk_overlap = 30\n",
    "  )\n",
    "\n",
    "  # pass in the file to be split\n",
    "  # output is iterable\n",
    "  texts = text_splitter.split_text(file)\n",
    "  return texts\n",
    "\n",
    "def clean_text(text) :\n",
    "    # replace /n with whitespace\n",
    "    # text = text.replace(\"\\n\",\"\")\n",
    "\n",
    "    # replace double whitespace with single whitespace    \n",
    "    text = text.replace(\"  \", \" \")\n",
    "\n",
    "    # remove [i], where i is the number inside of a reference point throughout the research paper\n",
    "    text = re.sub(\"\\[.*?\\]\", '', text)\n",
    "\n",
    "    # exclude the References portion of the research paper\n",
    "    # some References are not exact (due to pdf rendering or page text alignment) so we need to do fuzzy matching\n",
    "    token_end = \"ACKNOWLEDGEMENTS\"\n",
    "    try : \n",
    "        stripped_text_v1 = text.split(token_end, 1)[0]\n",
    "    except : \n",
    "        token_end = \"Acknowledgements\"\n",
    "        stripped_text_v1 = text.split(token_end, 1)[0]\n",
    "\n",
    "    # exclude the Introduction portion of the research paper\n",
    "    # some Introduction are not exact (due to pdf rendering or page text alignment) so we need to do fuzzy matching\n",
    "    token_intro = \"ABSTRACT\"\n",
    "    try :\n",
    "        stripped_text_v2 = stripped_text_v1.split(token_intro, 1)[1]\n",
    "    except :\n",
    "        token_intro = \"Abstract\"\n",
    "        stripped_text_v2 = stripped_text_v1.split(token_intro, 1)[1]\n",
    "  \n",
    "\n",
    "    return stripped_text_v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b06609a",
   "metadata": {},
   "source": [
    "## Define a path to a folder with all of the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bcabdae-9760-4577-acdf-ef023021fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pdf_files/*.pdf'\n",
    "pdf_list = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c0fdbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['research_paper\\\\ECGBERT.pdf',\n",
       " 'research_paper\\\\enhance_instruction.pdf',\n",
       " 'research_paper\\\\FLAN.pdf',\n",
       " 'research_paper\\\\FLANv2.pdf',\n",
       " 'research_paper\\\\FLAN_betterdesign.pdf',\n",
       " 'research_paper\\\\HomoGCL.pdf',\n",
       " 'research_paper\\\\KiDS-1000.pdf',\n",
       " 'research_paper\\\\LoRA.pdf',\n",
       " 'research_paper\\\\Med-MMHL.pdf',\n",
       " 'research_paper\\\\MIXALIME.pdf',\n",
       " 'research_paper\\\\MoleCLUEs.pdf',\n",
       " 'research_paper\\\\OpenGSL.pdf',\n",
       " 'research_paper\\\\PoET.pdf',\n",
       " 'research_paper\\\\self_instruct.pdf',\n",
       " 'research_paper\\\\SpreadDetect.pdf',\n",
       " 'research_paper\\\\UltraLlama.pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "155040ae",
   "metadata": {},
   "source": [
    "## Creating prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66563951-8767-4ca4-b623-559fc7641001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\pdf_parser\\venv_pdf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)16ebc/.gitattributes: 100%|██████████| 737/737 [00:00<00:00, 82.5kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 11.3kB/s]\n",
      "Downloading (…)b6b5d16ebc/README.md: 8.65kB [00:00, 1.08MB/s]\n",
      "Downloading (…)b5d16ebc/config.json: 100%|██████████| 571/571 [00:00<00:00, 32.8kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 12.9kB/s]\n",
      "Downloading (…)ebc/data_config.json: 25.5kB [00:00, ?B/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:22<00:00, 19.3MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 4.02kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 29.8kB/s]\n",
      "Downloading (…)16ebc/tokenizer.json: 466kB [00:00, 1.24MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 45.8kB/s]\n",
      "Downloading (…)6ebc/train_script.py: 13.9kB [00:00, ?B/s]\n",
      "Downloading (…)b6b5d16ebc/vocab.txt: 232kB [00:00, 566kB/s] \n",
      "Downloading (…)5d16ebc/modules.json: 100%|██████████| 229/229 [00:00<00:00, 25.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "final_chunk_list = []\n",
    "\n",
    "prompt = \"Based on the text, can you generate 5 different question and answer pairs in the following format.\\nAnswer format: \\n1. QUESTION : {vicuna to insert question}, \\nANSWER: {vicuna to insert answer} \\nText :\"\n",
    "\n",
    "\n",
    "for pdf in pdf_list :\n",
    "    # load pdf\n",
    "    document = extract_text_with_ocr(pdf)\n",
    "\n",
    "    # preprocess\n",
    "    clean_doc = clean_text(document)\n",
    "\n",
    "  # chunk doc\n",
    "    chunked_text_list = text_chunker(clean_doc)\n",
    "\n",
    "    for chunk in chunked_text_list :\n",
    "        final_chunk = prompt + \"\\n\" + chunk\n",
    "        final_chunk_list.append(final_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570c24d3-6365-4588-8fe3-5d9464802445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the text, can you generate 10 different question and answer pairs in the following format.\\nAnswer format: \\n1. QUESTION : {vicuna to insert question}, \\nANSWER: {vicuna to insert answer} \\nText :\\nin the medical field, current ecg signal analysis approaches rely on supervised deep neural networks trained for specific tasks that require substantial amounts of labeled data. however, our paper introduces ecgbert, a self - supervised representation learning approach that unlocks the underlying language of ecgs. by unsupervised pre - training of the model, we mitigate challenges posed by the lack of well - labeled and curated medical data. ecgbert, inspired by advances in the area of natural language processing and large language models, can be fine - tuned with minimal additional layers for various ecg - based problems. through four tasks, including atrial fibrillation arrhythmia detection, heartbeat classification, sleep apnea detection, and user authentication, we demonstrate ecgbert ’ s potential to achieve state - of - the - art results on a wide variety of tasks. 1 introduction the centers for disease control ( cdc ) reported that heart disease is the leading cause of death in the united states 2022 ]. specifically, one person dies every 34 seconds rom cardiovascular disease and about 697, 000 people died from heart disease in 2020, which is one in every five deaths. the electrocardiogram ( ecg ) is the most essential bio - signal used by cardiolo : gists and physicians to keep track of heart activity and detect different heart - related diseases and is used by cardiologists and physicians. one of the most critical limitations of ecg signals is that it requires manual analysis and annotation. furthermore, the interpretation of the ecg signals varies rom physician to physician as different heart diseases are associated with complex patterns within the ecg which can be hard to detect. the resulting inconsistencies may affect diagnostic accuracy o : r the trust between the patient and the physician. therefore, to mitigate the aforementioned limitations in regard to manual ecg interpretation, several studies have proposed alternative ecg ana techniques to achieve higher accuracy in real - time. among these, deep learning - based approa ysis ches have recently gained traction in this domain [ pyakillya et al. | 2017 \\\\ | mousavi et al. | 2020 }. compared with machine learning - based approaches where features need to be extracted nara deep learning - based approaches automatically extract relevant features, allowing for improved performance given enough data and a sufficiently aa model. a result, deep learning techniques have been widely applied to the medical domain in recent years ‘ “ corresponding author. +'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chunk_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17374897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This paper explores a simple method for improving the zero-shot learning abilities\n",
      "of language models. We show that instruction tuning—finetuning language models\n",
      "on a collection of datasets described via instructions—substantially improves zero-\n",
      "shot performance on unseen tasks.\n",
      "\n",
      "We take a 137B parameter pretrained language model and instruction tune it on\n",
      "over 60 NLP datasets verbalized via natural language instruction templates. We\n",
      "evaluate this instruction-tuned model, which we call FLAN, on unseen task types.\n",
      "FLAN substantially improves the performance of its unmodified counterpart and\n",
      "surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even\n",
      "outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC,\n",
      "OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning\n",
      "datasets, model scale, and natural language instructions are key to the success of\n",
      "instruction tuning.\n",
      "\n",
      "Finetune on many tasks (“instruction-tuning”)\n",
      "\n",
      "Input (Commonsense Reasoning) | Input (Translation)\n",
      "Here is a goal: Get a cool sleep on Translate this sentence to Inference on unseen task type\n",
      "Soe Spanish: Input (Natural Language Inference)\n",
      "\n",
      "How would you accomplish this goal? The new office building ToT ISSVATISGS To RTCUSEN\n",
      "\n",
      "OPTIONS: SE es have learnt one lesson.\n",
      "\n",
      "a months.\n",
      "\n",
      "pneep stack of plow cases in ildge. Hypothesis: It's not certain how many\n",
      "-Keep stack of pillow cases in oven. Target > lessons you'll learn by your thirties.\n",
      "Target EI nuevo edificio de oficinas Does the premise entail the hypothesis?\n",
      "keep stack of pillow cases in fridge se construyé en tres meses. OPTIONS:\n",
      "\n",
      "yes ) (-it is not possible to tell\n",
      "\n",
      "Sentiment analysis tasks\n",
      "FLAN Response\n",
      "\n",
      "2109.01652v5  8 Feb 2022\n",
      "\n",
      "arXiv\n",
      "\n",
      "Coreference resolution tasks\n",
      "\n",
      "Itis not possible to tell\n",
      "\n",
      "{§) GPT-3 175B zero shot [J GPT-3 175B few-shot [J FLAN 137 zero-shot\n",
      "\n",
      "Performance\n",
      "on unseen\n",
      "task types\n",
      "\n",
      "Natural language inference | Reading Comprehension Closed-Book QA\n",
      "\n",
      "language model on a mixture of tasks phrased as instructions. At inference time, we evaluate on\n",
      "an unseen task type; for instance, we could evaluate the model on natural language inference (NLI)\n",
      "when no NLI tasks were seen during instruction tuning. Bottom: performance of zero-shot FLAN,\n",
      "compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning\n",
      "improved performance substantially out of ten we evaluate. NLI datasets: ANLI R1-R3, CB, RTE.\n",
      "Reading comprehension datasets: BoolQ, MultiRC, OBQA. Closed-book QA datasets: ARC-easy,\n",
      "ARC-challenge, NQ, TriviaQA.\n",
      "\n",
      "*Lead contributors. Author contributions]listed at end of paper|\n",
      "\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Language models (LMs) at scale, such as GPT-3 (Brown et al.}{2020), have been shown to perform\n",
      "few-shot learning remarkably well. They are , however. For\n",
      "example, GPT-3’s zero-shot performance is much worse than few-shot performance on tasks such as\n",
      "reading comprehension, question answering, and natural language inference. One potential reason\n",
      "\n",
      "is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not\n",
      "similar to the format of the pretraining data.\n",
      "In this paper, we explore a simple method to improve the zero-shot performance of large language\n",
      "models, which would expand their reach to a broader audience. We\n",
      "\n",
      ", such as “Js the sentiment of this movie review\n",
      "positive or negative?” or “Translate ‘how are you’ into Chinese.” We take a pretrained language\n",
      "model of 137B parameters and perform instruction tuning—finetuning the model on a mixture of\n",
      "more than 60 NLP datasets expressed via natural language instructions. We refer to this resulting\n",
      "model as FLAN, for Finetuned Language Net.\n",
      "\n",
      "To evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters\n",
      "based on their task types and hold out each cluster for evaluation while instruction tuning FLAN\n",
      "natural language inference, we instruction tune the model on a range of other NLP tasks such as\n",
      "commonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has\n",
      "\n",
      "we then evaluate its ability to\n",
      "perform zero-shot natural language inference.\n",
      "\n",
      "Our evaluations show that FLAN substantially improves the zero-shot performance of the base\n",
      "137B-parameter model. FLAN’s zero-shot also outperforms 175B-parameter GPT-3’s zero-shot on 20\n",
      "of 25 datasets that we evaluate, and even outperforms GPT-3’s few-shot by a large margin on ANLI,\n",
      "RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we find that\n",
      "\n",
      "and that the\n",
      "\n",
      "b\n",
      "\n",
      "of both the pretrain—finetune and prompting paradigms by using supervision via finetuning to\n",
      "improve language model’s responses to inference-time text interactions. Our empirical results\n",
      "demonstrate promising abilities of language models to perform tasks described purely via instructions.\n",
      "Source code for loading the instruction tuning dataset used for FLAN is publicly available at\n",
      "\n",
      "https: //github.com/google-research/ flan}\n",
      "\n",
      "(A) Pretrain—finetune (BERT, T5)\n",
      "\n",
      "(a >)\n",
      "Fink An —— ontack A (C) Instruction tuning (FLAN)\n",
      "\n",
      "* Typically requires many Instruction-tune on\n",
      "\n",
      "task-specific examples 5 Inference\n",
      "+ Gne specialized model many tasks: = —> on task A\n",
      "a for each task ) Tees\n",
      "Model leams to perform Inference on\n",
      ". many tasks via natura unseen tas\n",
      "(B) Prompting (GPT-3) L language instructions\n",
      "~\n",
      "\n",
      "Improve performance\n",
      "\n",
      "via few-shot prompting\n",
      "Pretrained or prompt engineering Inference\n",
      "LM on task A\n",
      "\n",
      "/\n",
      "\n",
      "\n",
      "2 FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING\n",
      "\n",
      "The motivation of instruction tuning is to improve the ability of language models to respond to NLP\n",
      "instructions. The idea is that by using supervision to teach an LM to perform tasks described via\n",
      "instructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate\n",
      "performance on unseen tasks, we group datasets into clusters by task type and hold out each task\n",
      "cluster for evaluation while instruction tuning on all remaining clusters.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "2.1 TASKS & TEMPLATES\n",
      "\n",
      "As creating an instruction tuning dataset with many tasks from scratch would be resource-intensive,\n",
      "we transform existing datasets from the research community into an instructional format. We\n",
      "aggregate 62 text datasets that are publicly available on Tensorflow Datasets, including both language\n",
      "each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are\n",
      "of the same task type. Descriptions, sizes, and examples of each dataset are shown in Appendix\n",
      "\n",
      "Natural lan inference ) (commonsense|/ Sentiment Paraphrase ) (Glosed-book QA) ( Struct to text )( Translation\n",
      "(7 datasets) (4datasets) || (4datasets) || (4 datasets) (3 datasets) (4datasets) || (8 datasets)\n",
      "\n",
      "(ANLI(R7-R3))C RTE || CoPA |} IMDB )||( MRPC__)}| (ARC (easyichal)) |] (CommonGen ) | | (Paracraw! ENIDE )\n",
      "(cB )C__SNLI_)} |(HellaSwag )]|(_Sentt40_)}|( aap )|{CNQ__)||(C_DART _)|| (Paracrawi ENes )\n",
      "( MNLI_)C_ WNL DJJ PIQA +) //C_sst-2__)/|( Paws )||(TOA _)||(C E2ENLG_)/| (Paracraw ENFR)\n",
      "\n",
      "QNLI (StoryCloze)}|(__Yelp J ((_—STS-B +) (_WEBNLG )\n",
      "\n",
      "WMT-16 EN/DE\n",
      "Reading comp. | (Read. comp. w/| ( Coreference Misc. es (WMT-16 ENIFI)\n",
      "(5 datasets) commonsense (3 datasets) (7 datasets) (11 datasets)\n",
      "\n",
      "BoolQ (2 datasets) || (DPR CToGA)CTREC)| | (AESLC—) (Mulli-News ) (SamSum_)\n",
      "(DROP )(SQuAD)| | (‘CosmosQA )| | (Winogrande QuAC_)CCoLA AG News _)( Newsroom.) (Wiki Lingua EN) | | (WMT-16 EN/RU )\n",
      "\n",
      "=< | (WICC Math) | CCNN-DM_) (Opin-Abs:iOebate) XSum__) 7\n",
      "MultiRC (-ReCorD )||( wsc273 (wurrt6 ENTR)\n",
      "\n",
      "Fix Punctuation (NLG) )) | C Gigaword _) (Opin-Abs: Movie )\n",
      "\n",
      "\n",
      "For each dataset, we manually compose ten unique templates that use natural language instructions\n",
      "to describe the task for that dataset. While most of the ten templates describe the original task, to\n",
      "increase diversity, for each dataset we also include up to three templates that “turned the task around,”\n",
      "(e.g., for sentiment classification we include templates asking to generate a movie review). We\n",
      "then instruction tune a pretrained language model on the mixture of all datasets, with examples in\n",
      "each dataset formatted via a randomly selected instruction template for that dataset. Figur\n",
      "multiple instruction templates for a natural language inference dataset.\n",
      "\n",
      "Premise Template 1 Template 3\n",
      "Russian cosmonaut Valery Polyakov\n",
      "set the record for the longest\n",
      "continuous amount of time spent in\n",
      "space, a staggering 438 days,\n",
      "\n",
      "\\ between 1994 and 1995.\n",
      "\n",
      "<premise>\n",
      "\n",
      "Read the following and\n",
      "determine if the hypothesis can\n",
      "be inferred from the premise:\n",
      "\n",
      "Premise: <premise>\n",
      "\n",
      "above, can we conclude that\n",
      "<hypothesis>?\n",
      "\n",
      "<options>\n",
      "\n",
      "Hypothesis: <hypothesis>\n",
      "\n",
      "Hypothesis => T I 2 <options>\n",
      "Russians hold the record for the emplate\n",
      "Us ingest stay in space. <premise>\n",
      "\n",
      "Target Can we infer the following? Template 4, ...\n",
      "Not ental oe Co)\n",
      "\n",
      "Not entailment <options>\n",
      "\n",
      "\n",
      "2.2 EVALUATION SPLITS\n",
      "\n",
      "We are interested in how FLAN performs on tasks not seen in instruction tuning, and so it is crucial to\n",
      "define what counts as an unseen task. Whereas some prior work defines unseen tasks by disallowing\n",
      "the same dataset to appear in training, we use a more conservative definition that leverages the\n",
      "if no datasets from any task clusters that D belongs to were seen during instruction tuning. For\n",
      "instance, if D is an entailment task, then no entailment datasets appeared in instruction tuning, and\n",
      "we instruction-tuned on all other clusters|'| Hence, to evaluate zero-shot FLAN on c task clusters, we\n",
      "instruction tune c models, where each model holds out a different task cluster for evaluation.\n",
      "\n",
      "'When evaluating on the read. comp. with commonsense cluster, both read. comp. and commonsense\n",
      "reasoning were dropped from instruction tuning. Conversely, the read. comp. with commonsense cluster was\n",
      "not used for instruction tuning when evaluating on read. comp. or commonsense reasoning. We also drop the\n",
      "paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "2.3. CLASSIFICATION WITH OPTIONS\n",
      "\n",
      "The output space for a given task is either one of several classes (classification) or free text (generation).\n",
      "As FLAN is an instruction-tuned version of a decoder-only language model, it naturally responds in\n",
      "free text, and so no further modifications are needed for generation tasks.\n",
      "\n",
      "For classification tasks, prior work used a rank classification approach where,\n",
      "\n",
      "for example, only two outputs (“yes” and “no”) are considered and the higher probability one is\n",
      "taken as the model’s prediction. Though this procedure is logically sound, it is imperfect in that the\n",
      "probability mass for answers may have an undesired distribution among ways of saying each answer\n",
      "(e.g., a large number of alternative ways of saying “yes” may lower the probability mass assigned\n",
      "to “yes”). Therefore, we include an options suffix, in which we append the token OPTIONS to the\n",
      "end of a classification task along with a list of the output classes for that task. This makes the model\n",
      "aware of which choices are desired when responding to classification tasks. Example use of options\n",
      "\n",
      "2.4 TRAINING DETAILS\n",
      "\n",
      "Model architecture and pretraining. In our experiments, we use LaAMDA-PT, a dense left-to-right,\n",
      "decoder-only transformer language model of 137B parameters (Thoppilan et al.|{2022). This model\n",
      "is pretrained on a collection of web documents (including those with computer code), dialog data,\n",
      "and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece\n",
      "\n",
      "library (Kudo & Richardson||2018). Around 10% of the pretraining data was non-English. Note that\n",
      "A-P’\n",
      "\n",
      "LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).\n",
      "\n",
      "Instruction tuning procedure. FLAN is the instruction-tuned version of LaMDA-PT. Our instruc-\n",
      "tion tuning pipeline mixes all datasets and randomly samples from each dataset. To balance the\n",
      "different sizes of datasets, we limit the number of training examples per dataset to 30k and follow\n",
      "the examples-proportional mixing scheme with a mixing rate maximum of 3kP]\n",
      "We finetune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor\n",
      "Optimizer with a learning rate of 3e-5. The input and target sequence lengths\n",
      "used in finetuning are 1024 and 256, respectively. We use packing (Raffel et al.|[2020) to combine\n",
      "multiple training examples into a single sequence, separating inputs from targets using a special EOS\n",
      "token. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations,\n",
      "we report results on the final checkpoint trained for 30k steps.\n",
      "\n",
      "3. RESULTS\n",
      "\n",
      "We evaluate FLAN on natural language inference, reading comprehension, closed-book QA, transla-\n",
      "tion, commonsense reasoning, coreference resolution, and struct-to-text. As described in we\n",
      "evaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for\n",
      "evaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses\n",
      "a different checkpoint). For each dataset, we evaluate the mean of performance on all templates,\n",
      "which proxies the expected performance given a typical natural language instruction. As a dev set is\n",
      "\n",
      "sometimes available for manual prompt engineering (Brown et al./2020), for each dataset we also\n",
      "obtain the test set performance using the template with the best dev set performance.\n",
      "\n",
      "For comparison, we report zero and few-shot results for LaMDA-PT using the same prompts as\n",
      "GPT-3 (as LaMDA-PT is not suitable for natural instructions without instruction tuning). This\n",
      "baseline provides the most direct ablation of how much instruction tuning helps. Instruction tuning\n",
      "significantly improves LaMDA-PT on most datasets.\n",
      "\n",
      "We also show the zero-shot performances of GPT-3 175B and GLaM 64B/64E\n",
      "(Du et al.|[2021), as reported in their respective papers. With the best dev template, zero-shot FLAN\n",
      "outperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3’s few-shot performance\n",
      "on 10 datasets. With the best dev-template, zero-shot FLAN outperforms zero-shot GLaM on 13 of\n",
      "19 available datasets and one-shot GLaM on 11 of 19 datasets.\n",
      "\n",
      "7In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional\n",
      "sampling weight for examples in excess of 3,000.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Overall, we observe that instruction tuning is very effective on tasks naturally verbalized as instruc-\n",
      "tions (e.g., NLI, QA, translation, struct-to-text) and is less effective on tasks directly formulated as\n",
      "language modeling, where instructions would be largely redundant (e.g., commonsense reasoning\n",
      "Results on natural language inference, reading comprehension, closed-book QA, and translation are\n",
      "\n",
      "Natural lanquage inference\n",
      "\n",
      "ANLI R2 000%\n",
      "ANLI R3 e-@\n",
      "ANLI R1 e—@\n",
      "CB oO O-@ *\n",
      "RTE ) o-o *\n",
      "Reading comprehension\n",
      "MultiRC\n",
      "OBQA fo) ©. awe\n",
      "\n",
      "BoolQ @ We\n",
      "\n",
      "Closed-book QA\n",
      "\n",
      "Na-O o* 0\n",
      "ARC-c Q-.0-@ *\n",
      "TQA fe)\n",
      "ARC-e 00 O*\n",
      "Translation\n",
      "EN toRO- © ok 3k FLAN 137B\n",
      "EN to DE fe) * © LaMDA-PT137B\n",
      "EN to FR fe) 4 © GPT-3 175B\n",
      "FR to EN es Pd Es O GLaM 64B/64E\n",
      "° ;\n",
      "DE to EN O° te Supervised model\n",
      "f T T T T T T T T T 1\n",
      "0 20 40 60 80 100\n",
      "\n",
      "Zero-shot performance\n",
      "\n",
      "64B/64E on natural language inference, reading comprehension, closed-book QA, and translation.\n",
      "Performance of FLAN is the mean of up to 10 instructional templates per task. Supervised models\n",
      "were either T5, BERT, or translation models (specified in Tablein the Appendix).\n",
      "\n",
      "Natural language inference (NLI. On five NLI datasets, where a model must determine whether a\n",
      "hypothesis is true given some premise, FLAN outperforms all baselines by a large margin. As noted\n",
      "by (2020), perhaps one reason why GPT-3 struggles with NLI is that NLI examples are\n",
      "unlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased\n",
      "as a continuation of a sentence. For FLAN, we phrase NLI as the more natural question “Does\n",
      "<premise> mean that <hypothesis>?”, achieving much higher performance.\n",
      "\n",
      "Reading comprehension. On reading comprehension, where models are asked to answer a question\n",
      "about a provided passage, FLAN outperforms baselines for MultiRC and\n",
      "OBQA (Mihaylov et al.|/2018). On BoolQ (Clark et al.| 2019a), FLAN outperforms GPT-3 by a large\n",
      "margin, though LaMDA-PT already achieves high performance on BoolQ.\n",
      "\n",
      "Closed-book QA. For closed-book QA, which asks models to answer questions about the world\n",
      "without access to specific information containing the answer, FLAN outperforms GPT-3 on all four\n",
      "datasets. Compared to GLaM, FLAN has better performance on ARC-e and ARC-c (Clark et al.\n",
      "\n",
      "2018), and slightly lower performance on NQ (Lee et al.|/2019} [Kwiatkowski et al.|[2019) and TQA\n",
      "(Joshi et al.|/2017).\n",
      "\n",
      "Translation. Similar to GPT-3, the training data for LAMDA-PT is around 90% English and includes\n",
      "some text in other languages that was not specifically used to train the model to perform machine\n",
      "translation. We also evaluate FLAN’s performance on machine translation for the three datasets\n",
      "\n",
      "evaluated in the GPT-3 paper: French-English from WMT’ 14 (Bojar et al.||2014), and German—\n",
      "\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "English and Romanian—English from WMT’ 16 ). Compared with GPT-3, FLAN\n",
      "outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in\n",
      "most cases. Similar to GPT-3, FLAN shows strong results for translating into English and compares\n",
      "favorably against supervised translation baselines. Translating from English into other languages,\n",
      "however, was relatively weaker, as might be expected given that FLAN uses an English sentencepiece\n",
      "tokenizer and that the majority of pretraining data is English.\n",
      "\n",
      "Additional tasks. Although we see strong results for the above task clusters, one limitation with\n",
      "instruction tuning is that it does not improve performance for many language modeling tasks (e.g.,\n",
      "commonsense reasoning or coreference resolution tasks formulated as sentence completions). For\n",
      "seven commonsense reasoning and coreference resolution tasks (see Table jin the Appendix), FLAN\n",
      "only outperforms LaMDA-PT on three of the seven tasks. This negative result indicates that when the\n",
      "downstream task is the same as the original language modeling pre-training objective (i.e., in cases\n",
      "where instructions are largely redundant), instruction tuning is not useful. Finally, we report results for\n",
      "sentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which\n",
      "GPT-3 results are not available, in Tablein the Appendix. Generally, zero-shot FLAN\n",
      "outperforms zero-shot LaMDA-PT and is comparable with or better than few-shot LaMDA-PT.\n",
      "\n",
      "4 ABLATION STUDIES & FURTHER ANALYSIS\n",
      "\n",
      "4.1 NUMBER OF INSTRUCTION TUNING CLUSTERS\n",
      "\n",
      "As the core question of our paper asks how instruction tuning improves a model’s zero-shot perfor-\n",
      "mance on unseen tasks, in this first ablation we examine how performance is affected by the number\n",
      "of clusters and tasks used in instruction tuning. For this setup, we hold out NLI, closed-book QA, and\n",
      "commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction\n",
      "tuning|’) We show results for one to seven instruction tuning clusters, where clusters are added in\n",
      "decreasing order of number of tasks per cluster.\n",
      "\n",
      "held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the\n",
      "exception of the sentiment analysis cluster), confirming the benefits of our proposed instruction\n",
      "tuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for\n",
      "the seven clusters we test, the performance does not appear to saturate, implying that performance\n",
      "may further improve with even more clusters added to instruction tuning. Of note, this ablation does\n",
      "not allow us to draw conclusions about which instruction tuning cluster contributes the most to each\n",
      "evaluation cluster, although we see minimal added value from the sentiment analysis cluster.\n",
      "\n",
      "Held-out clusters\n",
      "\n",
      "SL —Commonsense\n",
      "an\n",
      "on i aaa\n",
      "\n",
      "© 70 63.5\n",
      "(3)\n",
      "es 550 923. 592 608 619 — Average\n",
      "ES os ee NLI\n",
      "Ex, <\n",
      "s 3 50 a Closed-book QA\n",
      "o\n",
      "\n",
      "Cc\n",
      "a re} mi ool ake Lid: Giosed-baok GA ae\n",
      "\n",
      "30\n",
      "# clusters: 1 2 3 4 5 6 7\n",
      "(# datasets): (11) (20) (26) (30) (34) (37) (39)\n",
      ". a\n",
      "BP gF ot oh ce? oP\n",
      "ox oe? PF wer 0 Pond om\n",
      "GOT WA PK eo? ge a\n",
      "sy x x\n",
      "\n",
      "Clusters used for instruction tuning\n",
      "\n",
      "held-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag,\n",
      "PiQA, and StoryCloze. NLI: ANLI R1-R3, QNLI, RTE, SNLI, and WNLLI. Closed-book QA: ARC\n",
      "easy, ARC challenge, Natural Questions, and TriviaQA.\n",
      "\n",
      "3We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning\n",
      "in this ablation because they are too similar to NLI and commmonsense reasoning, respectively.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Performance on held-out tasks\n",
      "70\n",
      "\n",
      "4.2 SCALING LAWS\n",
      "\n",
      "As|[Brown et al.|(2020) shows that zero and\n",
      "\n",
      "few-shot capabilities of language models sub-\n",
      "stantially improve for larger models, we next\n",
      "explore how the benefits of instruction tuning\n",
      "are affected by model scale. Using the same\n",
      "cluster split as in the previous ablation study,\n",
      "we evaluate the effect of instruction tuning\n",
      "on models of size 422M, 2B, 8B, 68B, and\n",
      "137B parameters.\n",
      "\n",
      "Instruction tuning\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "Untuned model\n",
      "\n",
      "a\n",
      "Ss\n",
      "\n",
      "on 13 held-out tasks (%)\n",
      "B\n",
      "&\n",
      "\n",
      "Average zero-shot accuracy\n",
      "\n",
      "w\n",
      "i\n",
      "\n",
      "0.4B 2B 8B 68B 137B\n",
      "for the two models on the order of 100B pa-\n",
      "rameters, instruction tuning substantially im-\n",
      "proves performance on held-out tasks, as is\n",
      "expected given the prior results in our pa-\n",
      "per. The behavior on held-out tasks for the\n",
      "8B and smaller models, however, is thought-\n",
      "provoking—instruction tuning actually hurts performance on held-out tasks. One potential explana-\n",
      "tion for this result could be that for small-scale models, learning the ~40 tasks used during instruction\n",
      "tuning fills the entire model capacity, causing these models to perform worse on new tasks. Under\n",
      "this potential explanation, for the larger scale models, instruction tuning fills up some model capacity\n",
      "but also teaches these models how to follow instructions, allowing them to generalize to new tasks\n",
      "\n",
      "models generalize to new tasks, for small models it\n",
      "actually hurts generalization to unseen tasks, poten-\n",
      "tially because all model capacity is used to learn the\n",
      "mixture of instruction tuning tasks.\n",
      "\n",
      "with the remaining capacity.\n",
      "\n",
      "4.3. ROLE OF INSTRUCTIONS\n",
      "\n",
      "In a final ablation study, we explore the role of in-\n",
      "structions during finetuning, as one possibility is that\n",
      "\n",
      ". We hence consider two finetuning setups\n",
      "without instructions. In a no template setup, only inputs\n",
      "and outputs were given to the model (e.g., for transla-\n",
      "tion the input would be “The dog runs.” and the output\n",
      "would be “Le chien court.”’). In a dataset name setup,\n",
      "each input is prepended with the name of the task and\n",
      "dataset (e.g., for translation to French, the input would\n",
      "be “/Translation: WMT’ 14 to French] The dog runs.’).\n",
      "\n",
      "We compare these two ablations to FLAN’s finetun-\n",
      "\n",
      "FT: no instruction\n",
      "Eval: instruction\n",
      "\n",
      "FT: dataset name\n",
      "Eval: instruction\n",
      "\n",
      "FT: dataset name\n",
      "Eval: dataset name\n",
      "\n",
      "FT: instruction\n",
      "Eval: instruction\n",
      "(FLAN)\n",
      "\n",
      "20\n",
      "\n",
      "50\n",
      "Zero-shot performance\n",
      "(4 task cluster avg.)\n",
      "\n",
      "60\n",
      "\n",
      "\n",
      "els with instructions removed from finetun-\n",
      "\n",
      "ing procedure, which used natural instructions (e.g., °\n",
      "ing (FT).\n",
      "\n",
      "“Please translate this sentence to French: ‘The dog\n",
      "runs.””). We perform evaluations for four held-out clus-\n",
      "\n",
      "inference (because if we used no template, the model would not know what task to perform). For\n",
      "models finetuned on dataset name only, we report zero-shot performance for FLAN instructions as\n",
      "substantially worse than FLAN, indicating that training with instructions is crucial for zero-shot\n",
      "performance on unseen tasks.\n",
      "\n",
      "4.4 INSTRUCTIONS WITH FEW-SHOT EXEMPLARS\n",
      "\n",
      "So far, we have focused on instruction tuning in the zero-shot setting. Here, we study how instruction\n",
      "tuning can be used when The format for the\n",
      "few-shot setting builds on the zero-shot format. For some input x and output y, let instruct(z)\n",
      "denote the zero-shot instructions. Then, given k few-shot exemplars (2;, 4 ih and a new input\n",
      "x, the instruction format for the few-shot setting is “instruct(x1) © y; © instruct(x2) @ y2 ®@...®\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "instruct(a,) ® yx @instruct(2)”, where @ denotes string concatenation with a delimiter token inserted\n",
      "in between. At both training and inference time, exemplars are randomly drawn from the training set,\n",
      "and the number of exemplars is capped at 16 and such that the total sequence length is less than 960\n",
      "tokens. Our experiment uses the same task splits and evaluation procedure as such that few-shot\n",
      "exemplars for an unseen task are only used at inference time.\n",
      "\n",
      "with zero-shot FLAN. Exemplars are especially effective for tasks with large/complex output spaces,\n",
      "such as struct to text, translation, and closed-book QA, potentially because exemplars help the model\n",
      "better understand the output format. In addition, for all task clusters, standard deviation among\n",
      "\n",
      "templates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\n",
      "\n",
      "80.0 80.8 Zero-shot FLAN\n",
      "80 1\n",
      "liFew-shot FLAN\n",
      "\n",
      "Ae 63.8 O74\n",
      "g . 59.6 60.0\n",
      "8 54.7 I\n",
      "5 60 I 57 02\n",
      "E 1 49.4\n",
      "S\n",
      "e 39.2\n",
      "& 40 I\n",
      "31.0 33.0\n",
      "i\n",
      "20\n",
      "Task Cluster: NLI Read. Comp. Closed-Book QA Commonsense Coreference Translation _Struct to text\n",
      "# datasets: 7 5 3 4 2 3 4\n",
      "\n",
      "performance of instruction-tuned models. The orange bars indicate standard deviation among\n",
      "templates, averaged at the dataset level for each task cluster.\n",
      "\n",
      "4.5 INSTRUCTION TUNING FACILITATES PROMPT TUNING\n",
      "Untuned model\n",
      "\n",
      "As we’ve seen that instruction tuning improves the ability of @ Instruction-tuned model\n",
      "a model to respond to instructions, it follows that, if FLAN is 100 aval\n",
      "indeed more amenable to performing NLP tasks, then it should pea ie\n",
      "\n",
      "75 63.8\n",
      "\n",
      "50\n",
      "(Li & Liang] [2021\n",
      ". As further analysis, we train continuous prompts for eac\n",
      "\n",
      "25\n",
      "of the SuperGLUE (Wang et al.| tasks in accordance with\n",
      "the cluster splits from §2.2]such that when prompt-tuning on task\n",
      "\n",
      "T, no tasks in the same cluster as T were seen during instruction\n",
      "tuning. Our prompt tuning setup follows the procedure of|\n",
      "1) except that we use a prompt length of 10, weight F\n",
      "e-4, and did not use dropout on the attention scores; we When prompt tuning on a given\n",
      "found in preliminary experiments that these changes improved ataset, no tasks from the same\n",
      "\n",
      "the performance of LaAMDA-PT. cluster as that dataset were seen\n",
      "during instruction tuning. Perfor-\n",
      "\n",
      "for both using a fully-supervised training set and in a low-resource _ the SuperGLUE dev set.\n",
      "\n",
      "setting with only 32 training examples. We see that in all sce-\n",
      "\n",
      "narios, prompt tuning works better with FLAN than LaMDA-PT. In many cases, especially for the\n",
      "low-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over\n",
      "prompt tuning on the LaMDA-PT. This result exemplifies in another way how instruction tuning can\n",
      "result in a checkpoint that is more desirable for performing NLP tasks.\n",
      "\n",
      "Performance after\n",
      "prompt tuning\n",
      "\n",
      "0\n",
      "32 training Full training\n",
      "examples set\n",
      "\n",
      "models respond better to contin-\n",
      "uous inputs from prompt tuning.\n",
      "\n",
      "5 RELATED WORK\n",
      "\n",
      "Our work relates to several broad research areas including zero-shot learning, prompting, multi-task\n",
      "\n",
      "learning, and language models for NLP applications (Radford et al. Raffel et al.\n",
      "2021} |Li & Liang}|2021} inter alia). We describe\n",
      "\n",
      "et al} 2020} [Efrat & Levy||2020}/Aghajanyan et al.\n",
      "prior work for these broad areas in an extended related work section (Appendix [Dp, and here we\n",
      "describe two subareas narrower in scope that perhaps relate most closely to our work.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "The way we ask a model to respond to instructions is similar to QA-based task formulation\n",
      "et al.| 2016} |McCann et al. 018), which aims to unify NLP tasks by casting them as QA over a\n",
      "context. Though these methods are very similar to ours, they mostly focus on multi-task learning\n",
      "instead of zero-shot learning, and—as noted by|Li )—they are generally not motivated\n",
      "by using existing knowledge in pretrained LMs. Moreover, our work supercedes recent work such as\n",
      "\n",
      "Chai et al. (2020) and|Zhong et al. (2021) in terms of both model scale and scope of tasks.\n",
      "\n",
      "The success of language models has led to nascent research on the ability of models to follow\n",
      "instructions. Most recently, {Mishra et al.|(2021) finetune 140M parameter BART on instructions\n",
      "with few-shot exemplars, and evaluate its few-shot abilities on unseen tasks—this is similar to our\n",
      "few-shot instruction tuning result from §4-4] This promising result (as well as one from[Ye et al.|\n",
      "1), which does not emphasize instructions as much) suggests that finetuning on a collection of\n",
      "s improves few-shot performance on unseen tasks, even at a smaller model scale.\n",
      "finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a\n",
      "model of 11B parameters. At a model scale similar to ours, OpenAI’s InstructGPT models are trained\n",
      "via both finetuning and reinforcement learning to produce outputs that are more preferred by human\n",
      "\n",
      "raters (Ouyang et al.\n",
      "\n",
      "6 DISCUSSION\n",
      "\n",
      "Our paper has explored a simple question in zero-shot prompting: does finetuning a model on a\n",
      "collection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\n",
      "this question via instruction tuning, a simple method that combines appealing aspects of both\n",
      "the pretrain—finetune and prompting paradigms. Our instruction-tuned model, FLAN, improves\n",
      "performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\n",
      "we evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\n",
      "of instruction tuning task clusters, and, interestingly, that performance improvements from instruction\n",
      "tuning emerge only with sufficient model scale. Moreover, instruction tuning can be combined with\n",
      "other prompting methods such as few-shot prompting and prompt tuning.\n",
      "\n",
      "The diverse capabilities of language models at scale have drawn attention to the tradeoffs between\n",
      "specialist models (one model per task) and generalist models (one model for many tasks;{Arivazhagan|\n",
      "fet al.|[2019} [Pratap et al.||2020), for which our study has potential implications. Although one might\n",
      "expect labeled data to have the most natural role in improving specialist models, instruction tuning\n",
      "demonstrates how labeled data can be used to help large language models perform many, unseen\n",
      "tasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\n",
      "task-specific training is complementary to general language modeling and motivates further research\n",
      "on generalist models.\n",
      "\n",
      "As for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\n",
      "we try to use accepted categorizations in the literature), and we only explore the use of relatively\n",
      "short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers).\n",
      "A limitation for our evaluation is that individual examples might have appeared in the models’\n",
      "pretraining data, which includes web documents, though in post-hoc analysis (Appendix [Cp we do\n",
      "not find any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN\n",
      "137B makes it costly to serve. Future work on instruction tuning could include gathering/generating\n",
      "even more task clusters for finetuning, cross-lingual experiments, using FLAN to generate data for\n",
      "training downstream classifiers, and using finetuning to improve model behavior with respect to bias\n",
      "\n",
      "and fairness (Solaiman & Dennison] [202 1).\n",
      "\n",
      "7 CONCLUSIONS\n",
      "\n",
      "This paper has explored a simple method for improving the ability of language models at scale to\n",
      "perform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares\n",
      "favorably against GPT-3 and signals the potential ability for language models at scale to follow\n",
      "instructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot\n",
      "learning, and using labeled data to improve large language models.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "ETHICAL CONSIDERATIONS\n",
      "\n",
      "This work uses language models, for which the risks and potential harms are discussed in|Bender &\n",
      "Koller (2020), Brown et al. (2020), Bender et al.|(2021), Patterson et al., (2021), and others. As our\n",
      "contribution in this paper is not a pretrained language model itself but rather an empirical study of\n",
      "how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we\n",
      "additionally highlight two relevant ethical considerations. First, labeled datasets such as those we\n",
      "use for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot\n",
      "applications of the model on downstream tasks. And second, instruction-tuned models can potentially\n",
      "require less data and expertise to use; such lower barriers to access could increase both the benefits\n",
      "and associated risks of such models.\n",
      "\n",
      "ENVIRONMENTAL CONSIDERATIONS\n",
      "\n",
      "We use the same pretrained language models as{Austin et al.| (2021). The energy cost and carbon\n",
      "footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional\n",
      "instruction tuning gradient-steps for finetuning FLAN is less than 2% of the number of pretraining\n",
      "steps, and so the estimated additional energy cost is comparatively smaller.\n",
      "\n",
      "AUTHOR CONTRIBUTIONS\n",
      "\n",
      "Maarten Bosma conceived the original idea and implemented the first version of FLAN. Vincent Zhao\n",
      "prototyped the training and evaluation pipelines, as well as rank classification. Kelvin Guu proposed\n",
      "and implemented the idea of task clusters and evaluation using inter-cluster splits. Jason Wei, Maarten\n",
      "Bosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. Jason Wei, Vincent Zhao,\n",
      "and Adams Wei Yu conducted and managed most of the experiments. Jason Wei designed and ran the\n",
      "ablation studies. Jason Wei, Maarten Bosma, and Quoc V. Le wrote most of the paper. Jason Wei,\n",
      "Maarten Bosma, and Nan Du obtained the zero and few-shot baselines. Vincent Zhao and Kelvin Guu\n",
      "designed, implemented, and conducted the few-shot FLAN experiments. Maarten Bosma and Jason\n",
      "Wei ran the data contamination analysis. Brian Lester ran the prompt tuning experiments. Quoc V. Le\n",
      "and Andrew M. Dai advised, provided high-level guidance, and helped edit the paper.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = extract_text_with_ocr('research_paper/FLAN.pdf')\n",
    "pdf_clean = clean_text(pdf)\n",
    "print(pdf_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d9c2ea-9822-4e1a-a31b-d24381c5566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "FINETUNED LANGUAGE MODELS ARE ZERO-SHOT\n",
      "LEARNERS\n",
      "\n",
      "Jason Wei*, Maarten Bosma*, Vincent Y. Zhao*, Kelvin Guu*, Adams Wei Yu,\n",
      "Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le\n",
      "\n",
      "Google Research\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "This paper explores a simple method for improving the zero-shot learning abilities\n",
      "of language models. We show that instruction tuning—finetuning language models\n",
      "on a collection of datasets described via instructions—substantially improves zero-\n",
      "shot performance on unseen tasks.\n",
      "\n",
      "We take a 137B parameter pretrained language model and instruction tune it on\n",
      "over 60 NLP datasets verbalized via natural language instruction templates. We\n",
      "evaluate this instruction-tuned model, which we call FLAN, on unseen task types.\n",
      "FLAN substantially improves the performance of its unmodified counterpart and\n",
      "surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even\n",
      "outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC,\n",
      "OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning\n",
      "datasets, model scale, and natural language instructions are key to the success of\n",
      "instruction tuning.\n",
      "\n",
      "Finetune on many tasks (“instruction-tuning”)\n",
      "\n",
      "Input (Commonsense Reasoning) | Input (Translation)\n",
      "Here is a goal: Get a cool sleep on Translate this sentence to Inference on unseen task type\n",
      "Soe Spanish: Input (Natural Language Inference)\n",
      "\n",
      "How would you accomplish this goal? The new office building ToT ISSVATISGS To RTCUSEN\n",
      "\n",
      "OPTIONS: SE es have learnt one lesson.\n",
      "\n",
      "a months.\n",
      "\n",
      "pneep stack of plow cases in ildge. Hypothesis: It's not certain how many\n",
      "-Keep stack of pillow cases in oven. Target > lessons you'll learn by your thirties.\n",
      "Target EI nuevo edificio de oficinas Does the premise entail the hypothesis?\n",
      "keep stack of pillow cases in fridge se construyé en tres meses. OPTIONS:\n",
      "\n",
      "yes ) (-it is not possible to tell\n",
      "\n",
      "Sentiment analysis tasks\n",
      "FLAN Response\n",
      "\n",
      "2109.01652v5 [cs.CL] 8 Feb 2022\n",
      "\n",
      "arXiv\n",
      "\n",
      "Coreference resolution tasks\n",
      "\n",
      "Itis not possible to tell\n",
      "\n",
      "{§) GPT-3 175B zero shot [J GPT-3 175B few-shot [J FLAN 137 zero-shot\n",
      "\n",
      "Performance\n",
      "on unseen\n",
      "task types\n",
      "\n",
      "Natural language inference | Reading Comprehension Closed-Book QA\n",
      "\n",
      "Figure 1: Top: overview of instruction tuning and FLAN. Instruction tuning finetunes a pretrained\n",
      "language model on a mixture of tasks phrased as instructions. At inference time, we evaluate on\n",
      "an unseen task type; for instance, we could evaluate the model on natural language inference (NLI)\n",
      "when no NLI tasks were seen during instruction tuning. Bottom: performance of zero-shot FLAN,\n",
      "compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning\n",
      "improved performance substantially out of ten we evaluate. NLI datasets: ANLI R1-R3, CB, RTE.\n",
      "Reading comprehension datasets: BoolQ, MultiRC, OBQA. Closed-book QA datasets: ARC-easy,\n",
      "ARC-challenge, NQ, TriviaQA.\n",
      "\n",
      "*Lead contributors. Author contributions]listed at end of paper|\n",
      "\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Language models (LMs) at scale, such as GPT-3 (Brown et al.}{2020), have been shown to perform\n",
      "few-shot learning remarkably well. They are , however. For\n",
      "example, GPT-3’s zero-shot performance is much worse than few-shot performance on tasks such as\n",
      "reading comprehension, question answering, and natural language inference. One potential reason\n",
      "\n",
      "is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not\n",
      "similar to the format of the pretraining data.\n",
      "In this paper, we explore a simple method to improve the zero-shot performance of large language\n",
      "models, which would expand their reach to a broader audience. We\n",
      "\n",
      ", such as “Js the sentiment of this movie review\n",
      "positive or negative?” or “Translate ‘how are you’ into Chinese.” We take a pretrained language\n",
      "model of 137B parameters and perform instruction tuning—finetuning the model on a mixture of\n",
      "more than 60 NLP datasets expressed via natural language instructions. We refer to this resulting\n",
      "model as FLAN, for Finetuned Language Net.\n",
      "\n",
      "To evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters\n",
      "based on their task types and hold out each cluster for evaluation while instruction tuning FLAN\n",
      "on all other clusters. For example, as shown in Figure [I] to evaluate FLAN’s ability to perform\n",
      "natural language inference, we instruction tune the model on a range of other NLP tasks such as\n",
      "commonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has\n",
      "\n",
      "we then evaluate its ability to\n",
      "perform zero-shot natural language inference.\n",
      "\n",
      "Our evaluations show that FLAN substantially improves the zero-shot performance of the base\n",
      "137B-parameter model. FLAN’s zero-shot also outperforms 175B-parameter GPT-3’s zero-shot on 20\n",
      "of 25 datasets that we evaluate, and even outperforms GPT-3’s few-shot by a large margin on ANLI,\n",
      "RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we find that\n",
      "\n",
      "and that the\n",
      "\n",
      "b\n",
      "\n",
      "Instruction tuning is a simple method that, as depicted in Figure [2] combines appealing aspects\n",
      "of both the pretrain—finetune and prompting paradigms by using supervision via finetuning to\n",
      "improve language model’s responses to inference-time text interactions. Our empirical results\n",
      "demonstrate promising abilities of language models to perform tasks described purely via instructions.\n",
      "Source code for loading the instruction tuning dataset used for FLAN is publicly available at\n",
      "\n",
      "https: //github.com/google-research/ flan}\n",
      "\n",
      "(A) Pretrain—finetune (BERT, T5)\n",
      "\n",
      "(a >)\n",
      "Fink An —— ontack A (C) Instruction tuning (FLAN)\n",
      "\n",
      "* Typically requires many Instruction-tune on\n",
      "\n",
      "task-specific examples 5 Inference\n",
      "+ Gne specialized model many tasks: = —> on task A\n",
      "a for each task ) Tees\n",
      "Model leams to perform Inference on\n",
      ". many tasks via natura unseen tas\n",
      "(B) Prompting (GPT-3) L language instructions\n",
      "~\n",
      "\n",
      "Improve performance\n",
      "\n",
      "via few-shot prompting\n",
      "Pretrained or prompt engineering Inference\n",
      "LM on task A\n",
      "\n",
      "/\n",
      "\n",
      "Figure 2: Comparing instruction tuning with pretrain—finetune and prompting.\n",
      "\n",
      "2 FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING\n",
      "\n",
      "The motivation of instruction tuning is to improve the ability of language models to respond to NLP\n",
      "instructions. The idea is that by using supervision to teach an LM to perform tasks described via\n",
      "instructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate\n",
      "performance on unseen tasks, we group datasets into clusters by task type and hold out each task\n",
      "cluster for evaluation while instruction tuning on all remaining clusters.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "2.1 TASKS & TEMPLATES\n",
      "\n",
      "As creating an instruction tuning dataset with many tasks from scratch would be resource-intensive,\n",
      "we transform existing datasets from the research community into an instructional format. We\n",
      "aggregate 62 text datasets that are publicly available on Tensorflow Datasets, including both language\n",
      "understanding and language generation tasks, into a single mixture. Figure|3|shows these datasets—\n",
      "each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are\n",
      "of the same task type. Descriptions, sizes, and examples of each dataset are shown in Appendix\n",
      "\n",
      "Natural lan inference ) (commonsense|/ Sentiment Paraphrase ) (Glosed-book QA) ( Struct to text )( Translation\n",
      "(7 datasets) (4datasets) || (4datasets) || (4 datasets) (3 datasets) (4datasets) || (8 datasets)\n",
      "\n",
      "(ANLI(R7-R3))C RTE || CoPA |} IMDB )||( MRPC__)}| (ARC (easyichal)) |] (CommonGen ) | | (Paracraw! ENIDE )\n",
      "(cB )C__SNLI_)} |(HellaSwag )]|(_Sentt40_)}|( aap )|{CNQ__)||(C_DART _)|| (Paracrawi ENes )\n",
      "( MNLI_)C_ WNL DJJ PIQA +) //C_sst-2__)/|( Paws )||(TOA _)||(C E2ENLG_)/| (Paracraw ENFR)\n",
      "\n",
      "QNLI (StoryCloze)}|(__Yelp J ((_—STS-B +) (_WEBNLG )\n",
      "\n",
      "WMT-16 EN/DE\n",
      "Reading comp. | (Read. comp. w/| ( Coreference Misc. es (WMT-16 ENIFI)\n",
      "(5 datasets) commonsense (3 datasets) (7 datasets) (11 datasets)\n",
      "\n",
      "BoolQ (2 datasets) || (DPR CToGA)CTREC)| | (AESLC—) (Mulli-News ) (SamSum_)\n",
      "(DROP )(SQuAD)| | (‘CosmosQA )| | (Winogrande QuAC_)CCoLA AG News _)( Newsroom.) (Wiki Lingua EN) | | (WMT-16 EN/RU )\n",
      "\n",
      "=< | (WICC Math) | CCNN-DM_) (Opin-Abs:iOebate) XSum__) 7\n",
      "MultiRC (-ReCorD )||( wsc273 (wurrt6 ENTR)\n",
      "\n",
      "Fix Punctuation (NLG) )) | C Gigaword _) (Opin-Abs: Movie )\n",
      "\n",
      "Figure 3: Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).\n",
      "\n",
      "For each dataset, we manually compose ten unique templates that use natural language instructions\n",
      "to describe the task for that dataset. While most of the ten templates describe the original task, to\n",
      "increase diversity, for each dataset we also include up to three templates that “turned the task around,”\n",
      "(e.g., for sentiment classification we include templates asking to generate a movie review). We\n",
      "then instruction tune a pretrained language model on the mixture of all datasets, with examples in\n",
      "each dataset formatted via a randomly selected instruction template for that dataset. Figur\n",
      "multiple instruction templates for a natural language inference dataset.\n",
      "\n",
      "Premise Template 1 Template 3\n",
      "Russian cosmonaut Valery Polyakov\n",
      "set the record for the longest\n",
      "continuous amount of time spent in\n",
      "space, a staggering 438 days,\n",
      "\n",
      "\\ between 1994 and 1995.\n",
      "\n",
      "<premise>\n",
      "\n",
      "Read the following and\n",
      "determine if the hypothesis can\n",
      "be inferred from the premise:\n",
      "\n",
      "Premise: <premise>\n",
      "\n",
      "Based on the paragraph\n",
      "above, can we conclude that\n",
      "<hypothesis>?\n",
      "\n",
      "<options>\n",
      "\n",
      "Hypothesis: <hypothesis>\n",
      "\n",
      "Hypothesis => T I 2 <options>\n",
      "Russians hold the record for the emplate\n",
      "Us ingest stay in space. <premise>\n",
      "\n",
      "Target Can we infer the following? Template 4, ...\n",
      "Not ental oe Co)\n",
      "\n",
      "Not entailment <options>\n",
      "\n",
      "Figure 4: Multiple instruction templates describing a natural language inference task.\n",
      "\n",
      "2.2 EVALUATION SPLITS\n",
      "\n",
      "We are interested in how FLAN performs on tasks not seen in instruction tuning, and so it is crucial to\n",
      "define what counts as an unseen task. Whereas some prior work defines unseen tasks by disallowing\n",
      "the same dataset to appear in training, we use a more conservative definition that leverages the\n",
      "task clusters from Figure 3] In this work, we only consider dataset D unseen at evaluation time\n",
      "if no datasets from any task clusters that D belongs to were seen during instruction tuning. For\n",
      "instance, if D is an entailment task, then no entailment datasets appeared in instruction tuning, and\n",
      "we instruction-tuned on all other clusters|'| Hence, to evaluate zero-shot FLAN on c task clusters, we\n",
      "instruction tune c models, where each model holds out a different task cluster for evaluation.\n",
      "\n",
      "'When evaluating on the read. comp. with commonsense cluster, both read. comp. and commonsense\n",
      "reasoning were dropped from instruction tuning. Conversely, the read. comp. with commonsense cluster was\n",
      "not used for instruction tuning when evaluating on read. comp. or commonsense reasoning. We also drop the\n",
      "paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "2.3. CLASSIFICATION WITH OPTIONS\n",
      "\n",
      "The output space for a given task is either one of several classes (classification) or free text (generation).\n",
      "As FLAN is an instruction-tuned version of a decoder-only language model, it naturally responds in\n",
      "free text, and so no further modifications are needed for generation tasks.\n",
      "\n",
      "For classification tasks, prior work used a rank classification approach where,\n",
      "\n",
      "for example, only two outputs (“yes” and “no”) are considered and the higher probability one is\n",
      "taken as the model’s prediction. Though this procedure is logically sound, it is imperfect in that the\n",
      "probability mass for answers may have an undesired distribution among ways of saying each answer\n",
      "(e.g., a large number of alternative ways of saying “yes” may lower the probability mass assigned\n",
      "to “yes”). Therefore, we include an options suffix, in which we append the token OPTIONS to the\n",
      "end of a classification task along with a list of the output classes for that task. This makes the model\n",
      "aware of which choices are desired when responding to classification tasks. Example use of options\n",
      "is shown in the NLI and commonsense examples in Figure[]]\n",
      "\n",
      "2.4 TRAINING DETAILS\n",
      "\n",
      "Model architecture and pretraining. In our experiments, we use LaAMDA-PT, a dense left-to-right,\n",
      "decoder-only transformer language model of 137B parameters (Thoppilan et al.|{2022). This model\n",
      "is pretrained on a collection of web documents (including those with computer code), dialog data,\n",
      "and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece\n",
      "\n",
      "library (Kudo & Richardson||2018). Around 10% of the pretraining data was non-English. Note that\n",
      "A-P’\n",
      "\n",
      "LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).\n",
      "\n",
      "Instruction tuning procedure. FLAN is the instruction-tuned version of LaMDA-PT. Our instruc-\n",
      "tion tuning pipeline mixes all datasets and randomly samples from each dataset. To balance the\n",
      "different sizes of datasets, we limit the number of training examples per dataset to 30k and follow\n",
      "the examples-proportional mixing scheme with a mixing rate maximum of 3kP]\n",
      "We finetune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor\n",
      "Optimizer with a learning rate of 3e-5. The input and target sequence lengths\n",
      "used in finetuning are 1024 and 256, respectively. We use packing (Raffel et al.|[2020) to combine\n",
      "multiple training examples into a single sequence, separating inputs from targets using a special EOS\n",
      "token. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations,\n",
      "we report results on the final checkpoint trained for 30k steps.\n",
      "\n",
      "3. RESULTS\n",
      "\n",
      "We evaluate FLAN on natural language inference, reading comprehension, closed-book QA, transla-\n",
      "tion, commonsense reasoning, coreference resolution, and struct-to-text. As described in we\n",
      "evaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for\n",
      "evaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses\n",
      "a different checkpoint). For each dataset, we evaluate the mean of performance on all templates,\n",
      "which proxies the expected performance given a typical natural language instruction. As a dev set is\n",
      "\n",
      "sometimes available for manual prompt engineering (Brown et al./2020), for each dataset we also\n",
      "obtain the test set performance using the template with the best dev set performance.\n",
      "\n",
      "For comparison, we report zero and few-shot results for LaMDA-PT using the same prompts as\n",
      "GPT-3 (as LaMDA-PT is not suitable for natural instructions without instruction tuning). This\n",
      "baseline provides the most direct ablation of how much instruction tuning helps. Instruction tuning\n",
      "significantly improves LaMDA-PT on most datasets.\n",
      "\n",
      "We also show the zero-shot performances of GPT-3 175B and GLaM 64B/64E\n",
      "(Du et al.|[2021), as reported in their respective papers. With the best dev template, zero-shot FLAN\n",
      "outperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3’s few-shot performance\n",
      "on 10 datasets. With the best dev-template, zero-shot FLAN outperforms zero-shot GLaM on 13 of\n",
      "19 available datasets and one-shot GLaM on 11 of 19 datasets.\n",
      "\n",
      "7In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional\n",
      "sampling weight for examples in excess of 3,000.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Overall, we observe that instruction tuning is very effective on tasks naturally verbalized as instruc-\n",
      "tions (e.g., NLI, QA, translation, struct-to-text) and is less effective on tasks directly formulated as\n",
      "language modeling, where instructions would be largely redundant (e.g., commonsense reasoning\n",
      "and coreference resolution tasks that are formatted as finishing an incomplete sentence or paragraph).\n",
      "Results on natural language inference, reading comprehension, closed-book QA, and translation are\n",
      "summarized in Figure|[5]and described below.\n",
      "\n",
      "Natural lanquage inference\n",
      "\n",
      "ANLI R2 000%\n",
      "ANLI R3 e-@\n",
      "ANLI R1 e—@\n",
      "CB oO O-@ *\n",
      "RTE ) o-o *\n",
      "Reading comprehension\n",
      "MultiRC\n",
      "OBQA fo) ©. awe\n",
      "\n",
      "BoolQ @ We\n",
      "\n",
      "Closed-book QA\n",
      "\n",
      "Na-O o* 0\n",
      "ARC-c Q-.0-@ *\n",
      "TQA fe)\n",
      "ARC-e 00 O*\n",
      "Translation\n",
      "EN toRO- © ok 3k FLAN 137B\n",
      "EN to DE fe) * © LaMDA-PT137B\n",
      "EN to FR fe) 4 © GPT-3 175B\n",
      "FR to EN es Pd Es O GLaM 64B/64E\n",
      "° ;\n",
      "DE to EN O° te Supervised model\n",
      "f T T T T T T T T T 1\n",
      "0 20 40 60 80 100\n",
      "\n",
      "Zero-shot performance\n",
      "\n",
      "Figure 5: Zero-shot performance of FLAN compared to LaMDA-PT 137B, GPT-3 175B, and GLaM\n",
      "64B/64E on natural language inference, reading comprehension, closed-book QA, and translation.\n",
      "Performance of FLAN is the mean of up to 10 instructional templates per task. Supervised models\n",
      "were either T5, BERT, or translation models (specified in Table[2jand Table[I]in the Appendix).\n",
      "\n",
      "Natural language inference (NLI. On five NLI datasets, where a model must determine whether a\n",
      "hypothesis is true given some premise, FLAN outperforms all baselines by a large margin. As noted\n",
      "by (2020), perhaps one reason why GPT-3 struggles with NLI is that NLI examples are\n",
      "unlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased\n",
      "as a continuation of a sentence. For FLAN, we phrase NLI as the more natural question “Does\n",
      "<premise> mean that <hypothesis>?”, achieving much higher performance.\n",
      "\n",
      "Reading comprehension. On reading comprehension, where models are asked to answer a question\n",
      "about a provided passage, FLAN outperforms baselines for MultiRC and\n",
      "OBQA (Mihaylov et al.|/2018). On BoolQ (Clark et al.| 2019a), FLAN outperforms GPT-3 by a large\n",
      "margin, though LaMDA-PT already achieves high performance on BoolQ.\n",
      "\n",
      "Closed-book QA. For closed-book QA, which asks models to answer questions about the world\n",
      "without access to specific information containing the answer, FLAN outperforms GPT-3 on all four\n",
      "datasets. Compared to GLaM, FLAN has better performance on ARC-e and ARC-c (Clark et al.\n",
      "\n",
      "2018), and slightly lower performance on NQ (Lee et al.|/2019} [Kwiatkowski et al.|[2019) and TQA\n",
      "(Joshi et al.|/2017).\n",
      "\n",
      "Translation. Similar to GPT-3, the training data for LAMDA-PT is around 90% English and includes\n",
      "some text in other languages that was not specifically used to train the model to perform machine\n",
      "translation. We also evaluate FLAN’s performance on machine translation for the three datasets\n",
      "\n",
      "evaluated in the GPT-3 paper: French-English from WMT’ 14 (Bojar et al.||2014), and German—\n",
      "\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "English and Romanian—English from WMT’ 16 ). Compared with GPT-3, FLAN\n",
      "outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in\n",
      "most cases. Similar to GPT-3, FLAN shows strong results for translating into English and compares\n",
      "favorably against supervised translation baselines. Translating from English into other languages,\n",
      "however, was relatively weaker, as might be expected given that FLAN uses an English sentencepiece\n",
      "tokenizer and that the majority of pretraining data is English.\n",
      "\n",
      "Additional tasks. Although we see strong results for the above task clusters, one limitation with\n",
      "instruction tuning is that it does not improve performance for many language modeling tasks (e.g.,\n",
      "commonsense reasoning or coreference resolution tasks formulated as sentence completions). For\n",
      "seven commonsense reasoning and coreference resolution tasks (see Table jin the Appendix), FLAN\n",
      "only outperforms LaMDA-PT on three of the seven tasks. This negative result indicates that when the\n",
      "downstream task is the same as the original language modeling pre-training objective (i.e., in cases\n",
      "where instructions are largely redundant), instruction tuning is not useful. Finally, we report results for\n",
      "sentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which\n",
      "GPT-3 results are not available, in Table[2jand Table[T]in the Appendix. Generally, zero-shot FLAN\n",
      "outperforms zero-shot LaMDA-PT and is comparable with or better than few-shot LaMDA-PT.\n",
      "\n",
      "4 ABLATION STUDIES & FURTHER ANALYSIS\n",
      "\n",
      "4.1 NUMBER OF INSTRUCTION TUNING CLUSTERS\n",
      "\n",
      "As the core question of our paper asks how instruction tuning improves a model’s zero-shot perfor-\n",
      "mance on unseen tasks, in this first ablation we examine how performance is affected by the number\n",
      "of clusters and tasks used in instruction tuning. For this setup, we hold out NLI, closed-book QA, and\n",
      "commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction\n",
      "tuning|’) We show results for one to seven instruction tuning clusters, where clusters are added in\n",
      "decreasing order of number of tasks per cluster.\n",
      "\n",
      "Figure[6]shows these results. As expected, we observe that average performance across the three\n",
      "held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the\n",
      "exception of the sentiment analysis cluster), confirming the benefits of our proposed instruction\n",
      "tuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for\n",
      "the seven clusters we test, the performance does not appear to saturate, implying that performance\n",
      "may further improve with even more clusters added to instruction tuning. Of note, this ablation does\n",
      "not allow us to draw conclusions about which instruction tuning cluster contributes the most to each\n",
      "evaluation cluster, although we see minimal added value from the sentiment analysis cluster.\n",
      "\n",
      "Held-out clusters\n",
      "\n",
      "SL —Commonsense\n",
      "an\n",
      "on i aaa\n",
      "\n",
      "© 70 63.5\n",
      "(3)\n",
      "es 550 923. 592 608 619 — Average\n",
      "ES os ee NLI\n",
      "Ex, <\n",
      "s 3 50 a Closed-book QA\n",
      "o\n",
      "\n",
      "Cc\n",
      "a re} mi ool ake Lid: Giosed-baok GA ae\n",
      "\n",
      "30\n",
      "# clusters: 1 2 3 4 5 6 7\n",
      "(# datasets): (11) (20) (26) (30) (34) (37) (39)\n",
      ". a\n",
      "BP gF ot oh ce? oP\n",
      "ox oe? PF wer 0 Pond om\n",
      "GOT WA PK eo? ge a\n",
      "sy x x\n",
      "\n",
      "Clusters used for instruction tuning\n",
      "\n",
      "Figure 6: Adding additional task clusters to instruction tuning improves zero-shot performance on\n",
      "held-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag,\n",
      "PiQA, and StoryCloze. NLI: ANLI R1-R3, QNLI, RTE, SNLI, and WNLLI. Closed-book QA: ARC\n",
      "easy, ARC challenge, Natural Questions, and TriviaQA.\n",
      "\n",
      "3We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning\n",
      "in this ablation because they are too similar to NLI and commmonsense reasoning, respectively.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Performance on held-out tasks\n",
      "70\n",
      "\n",
      "4.2 SCALING LAWS\n",
      "\n",
      "As|[Brown et al.|(2020) shows that zero and\n",
      "\n",
      "few-shot capabilities of language models sub-\n",
      "stantially improve for larger models, we next\n",
      "explore how the benefits of instruction tuning\n",
      "are affected by model scale. Using the same\n",
      "cluster split as in the previous ablation study,\n",
      "we evaluate the effect of instruction tuning\n",
      "on models of size 422M, 2B, 8B, 68B, and\n",
      "137B parameters.\n",
      "\n",
      "Instruction tuning\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "Untuned model\n",
      "\n",
      "a\n",
      "Ss\n",
      "\n",
      "on 13 held-out tasks (%)\n",
      "B\n",
      "&\n",
      "\n",
      "Average zero-shot accuracy\n",
      "\n",
      "w\n",
      "i\n",
      "\n",
      "0.4B 2B 8B 68B 137B\n",
      "Figure [7] shows these results. We see that Model Size (# parameters)\n",
      "for the two models on the order of 100B pa-\n",
      "rameters, instruction tuning substantially im-\n",
      "proves performance on held-out tasks, as is\n",
      "expected given the prior results in our pa-\n",
      "per. The behavior on held-out tasks for the\n",
      "8B and smaller models, however, is thought-\n",
      "provoking—instruction tuning actually hurts performance on held-out tasks. One potential explana-\n",
      "tion for this result could be that for small-scale models, learning the ~40 tasks used during instruction\n",
      "tuning fills the entire model capacity, causing these models to perform worse on new tasks. Under\n",
      "this potential explanation, for the larger scale models, instruction tuning fills up some model capacity\n",
      "but also teaches these models how to follow instructions, allowing them to generalize to new tasks\n",
      "\n",
      "Figure 7: Whereas instruction tuning helps large\n",
      "models generalize to new tasks, for small models it\n",
      "actually hurts generalization to unseen tasks, poten-\n",
      "tially because all model capacity is used to learn the\n",
      "mixture of instruction tuning tasks.\n",
      "\n",
      "with the remaining capacity.\n",
      "\n",
      "4.3. ROLE OF INSTRUCTIONS\n",
      "\n",
      "In a final ablation study, we explore the role of in-\n",
      "structions during finetuning, as one possibility is that\n",
      "\n",
      ". We hence consider two finetuning setups\n",
      "without instructions. In a no template setup, only inputs\n",
      "and outputs were given to the model (e.g., for transla-\n",
      "tion the input would be “The dog runs.” and the output\n",
      "would be “Le chien court.”’). In a dataset name setup,\n",
      "each input is prepended with the name of the task and\n",
      "dataset (e.g., for translation to French, the input would\n",
      "be “/Translation: WMT’ 14 to French] The dog runs.’).\n",
      "\n",
      "We compare these two ablations to FLAN’s finetun-\n",
      "\n",
      "FT: no instruction\n",
      "Eval: instruction\n",
      "\n",
      "FT: dataset name\n",
      "Eval: instruction\n",
      "\n",
      "FT: dataset name\n",
      "Eval: dataset name\n",
      "\n",
      "FT: instruction\n",
      "Eval: instruction\n",
      "(FLAN)\n",
      "\n",
      "20\n",
      "\n",
      "50\n",
      "Zero-shot performance\n",
      "(4 task cluster avg.)\n",
      "\n",
      "60\n",
      "\n",
      "Figure 8: Ablation study result using mod-\n",
      "\n",
      "els with instructions removed from finetun-\n",
      "\n",
      "ing procedure, which used natural instructions (e.g., °\n",
      "ing (FT).\n",
      "\n",
      "“Please translate this sentence to French: ‘The dog\n",
      "runs.””). We perform evaluations for four held-out clus-\n",
      "\n",
      "ters from Figure [5] For the no template setup, we used the FLAN instructions during zero-shot\n",
      "inference (because if we used no template, the model would not know what task to perform). For\n",
      "models finetuned on dataset name only, we report zero-shot performance for FLAN instructions as\n",
      "well as using the dataset name. Figure|8]shows the results—both ablation configurations performed\n",
      "substantially worse than FLAN, indicating that training with instructions is crucial for zero-shot\n",
      "performance on unseen tasks.\n",
      "\n",
      "4.4 INSTRUCTIONS WITH FEW-SHOT EXEMPLARS\n",
      "\n",
      "So far, we have focused on instruction tuning in the zero-shot setting. Here, we study how instruction\n",
      "tuning can be used when The format for the\n",
      "few-shot setting builds on the zero-shot format. For some input x and output y, let instruct(z)\n",
      "denote the zero-shot instructions. Then, given k few-shot exemplars (2;, 4 ih and a new input\n",
      "x, the instruction format for the few-shot setting is “instruct(x1) © y; © instruct(x2) @ y2 ®@...®\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "instruct(a,) ® yx @instruct(2)”, where @ denotes string concatenation with a delimiter token inserted\n",
      "in between. At both training and inference time, exemplars are randomly drawn from the training set,\n",
      "and the number of exemplars is capped at 16 and such that the total sequence length is less than 960\n",
      "tokens. Our experiment uses the same task splits and evaluation procedure as such that few-shot\n",
      "exemplars for an unseen task are only used at inference time.\n",
      "\n",
      "As shown in Figure[9} few-shot exemplars improve the performance on all task clusters, compared\n",
      "with zero-shot FLAN. Exemplars are especially effective for tasks with large/complex output spaces,\n",
      "such as struct to text, translation, and closed-book QA, potentially because exemplars help the model\n",
      "better understand the output format. In addition, for all task clusters, standard deviation among\n",
      "\n",
      "templates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\n",
      "\n",
      "80.0 80.8 Zero-shot FLAN\n",
      "80 1\n",
      "liFew-shot FLAN\n",
      "\n",
      "Ae 63.8 O74\n",
      "g . 59.6 60.0\n",
      "8 54.7 I\n",
      "5 60 I 57 02\n",
      "E 1 49.4\n",
      "S\n",
      "e 39.2\n",
      "& 40 I\n",
      "31.0 33.0\n",
      "i\n",
      "20\n",
      "Task Cluster: NLI Read. Comp. Closed-Book QA Commonsense Coreference Translation _Struct to text\n",
      "# datasets: 7 5 3 4 2 3 4\n",
      "\n",
      "Figure 9: Adding few-shot exemplars to FLAN is a complementary method for improving the\n",
      "performance of instruction-tuned models. The orange bars indicate standard deviation among\n",
      "templates, averaged at the dataset level for each task cluster.\n",
      "\n",
      "4.5 INSTRUCTION TUNING FACILITATES PROMPT TUNING\n",
      "Untuned model\n",
      "\n",
      "As we’ve seen that instruction tuning improves the ability of @ Instruction-tuned model\n",
      "a model to respond to instructions, it follows that, if FLAN is 100 aval\n",
      "indeed more amenable to performing NLP tasks, then it should pea ie\n",
      "\n",
      "75 63.8\n",
      "\n",
      "50\n",
      "(Li & Liang] [2021\n",
      ". As further analysis, we train continuous prompts for eac\n",
      "\n",
      "25\n",
      "of the SuperGLUE (Wang et al.| tasks in accordance with\n",
      "the cluster splits from §2.2]such that when prompt-tuning on task\n",
      "\n",
      "T, no tasks in the same cluster as T were seen during instruction\n",
      "tuning. Our prompt tuning setup follows the procedure of|\n",
      "1) except that we use a prompt length of 10, weight F\n",
      "e-4, and did not use dropout on the attention scores; we When prompt tuning on a given\n",
      "found in preliminary experiments that these changes improved ataset, no tasks from the same\n",
      "\n",
      "the performance of LaAMDA-PT. cluster as that dataset were seen\n",
      "during instruction tuning. Perfor-\n",
      "\n",
      "Figure{[0|shows the results of these prompt tuning experiments mance shown is the average on\n",
      "for both using a fully-supervised training set and in a low-resource _ the SuperGLUE dev set.\n",
      "\n",
      "setting with only 32 training examples. We see that in all sce-\n",
      "\n",
      "narios, prompt tuning works better with FLAN than LaMDA-PT. In many cases, especially for the\n",
      "low-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over\n",
      "prompt tuning on the LaMDA-PT. This result exemplifies in another way how instruction tuning can\n",
      "result in a checkpoint that is more desirable for performing NLP tasks.\n",
      "\n",
      "Performance after\n",
      "prompt tuning\n",
      "\n",
      "0\n",
      "32 training Full training\n",
      "examples set\n",
      "\n",
      "Figure 10: __ Instruction-tuned\n",
      "models respond better to contin-\n",
      "uous inputs from prompt tuning.\n",
      "\n",
      "5 RELATED WORK\n",
      "\n",
      "Our work relates to several broad research areas including zero-shot learning, prompting, multi-task\n",
      "\n",
      "learning, and language models for NLP applications (Radford et al. Raffel et al.\n",
      "2021} |Li & Liang}|2021} inter alia). We describe\n",
      "\n",
      "et al} 2020} [Efrat & Levy||2020}/Aghajanyan et al.\n",
      "prior work for these broad areas in an extended related work section (Appendix [Dp, and here we\n",
      "describe two subareas narrower in scope that perhaps relate most closely to our work.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "The way we ask a model to respond to instructions is similar to QA-based task formulation\n",
      "et al.| 2016} |McCann et al. 018), which aims to unify NLP tasks by casting them as QA over a\n",
      "context. Though these methods are very similar to ours, they mostly focus on multi-task learning\n",
      "instead of zero-shot learning, and—as noted by|Li )—they are generally not motivated\n",
      "by using existing knowledge in pretrained LMs. Moreover, our work supercedes recent work such as\n",
      "\n",
      "Chai et al. (2020) and|Zhong et al. (2021) in terms of both model scale and scope of tasks.\n",
      "\n",
      "The success of language models has led to nascent research on the ability of models to follow\n",
      "instructions. Most recently, {Mishra et al.|(2021) finetune 140M parameter BART on instructions\n",
      "with few-shot exemplars, and evaluate its few-shot abilities on unseen tasks—this is similar to our\n",
      "few-shot instruction tuning result from §4-4] This promising result (as well as one from[Ye et al.|\n",
      "1), which does not emphasize instructions as much) suggests that finetuning on a collection of\n",
      "s improves few-shot performance on unseen tasks, even at a smaller model scale.\n",
      "finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a\n",
      "model of 11B parameters. At a model scale similar to ours, OpenAI’s InstructGPT models are trained\n",
      "via both finetuning and reinforcement learning to produce outputs that are more preferred by human\n",
      "\n",
      "raters (Ouyang et al.\n",
      "\n",
      "6 DISCUSSION\n",
      "\n",
      "Our paper has explored a simple question in zero-shot prompting: does finetuning a model on a\n",
      "collection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\n",
      "this question via instruction tuning, a simple method that combines appealing aspects of both\n",
      "the pretrain—finetune and prompting paradigms. Our instruction-tuned model, FLAN, improves\n",
      "performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\n",
      "we evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\n",
      "of instruction tuning task clusters, and, interestingly, that performance improvements from instruction\n",
      "tuning emerge only with sufficient model scale. Moreover, instruction tuning can be combined with\n",
      "other prompting methods such as few-shot prompting and prompt tuning.\n",
      "\n",
      "The diverse capabilities of language models at scale have drawn attention to the tradeoffs between\n",
      "specialist models (one model per task) and generalist models (one model for many tasks;{Arivazhagan|\n",
      "fet al.|[2019} [Pratap et al.||2020), for which our study has potential implications. Although one might\n",
      "expect labeled data to have the most natural role in improving specialist models, instruction tuning\n",
      "demonstrates how labeled data can be used to help large language models perform many, unseen\n",
      "tasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\n",
      "task-specific training is complementary to general language modeling and motivates further research\n",
      "on generalist models.\n",
      "\n",
      "As for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\n",
      "we try to use accepted categorizations in the literature), and we only explore the use of relatively\n",
      "short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers).\n",
      "A limitation for our evaluation is that individual examples might have appeared in the models’\n",
      "pretraining data, which includes web documents, though in post-hoc analysis (Appendix [Cp we do\n",
      "not find any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN\n",
      "137B makes it costly to serve. Future work on instruction tuning could include gathering/generating\n",
      "even more task clusters for finetuning, cross-lingual experiments, using FLAN to generate data for\n",
      "training downstream classifiers, and using finetuning to improve model behavior with respect to bias\n",
      "\n",
      "and fairness (Solaiman & Dennison] [202 1).\n",
      "\n",
      "7 CONCLUSIONS\n",
      "\n",
      "This paper has explored a simple method for improving the ability of language models at scale to\n",
      "perform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares\n",
      "favorably against GPT-3 and signals the potential ability for language models at scale to follow\n",
      "instructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot\n",
      "learning, and using labeled data to improve large language models.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "ETHICAL CONSIDERATIONS\n",
      "\n",
      "This work uses language models, for which the risks and potential harms are discussed in|Bender &\n",
      "Koller (2020), Brown et al. (2020), Bender et al.|(2021), Patterson et al., (2021), and others. As our\n",
      "contribution in this paper is not a pretrained language model itself but rather an empirical study of\n",
      "how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we\n",
      "additionally highlight two relevant ethical considerations. First, labeled datasets such as those we\n",
      "use for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot\n",
      "applications of the model on downstream tasks. And second, instruction-tuned models can potentially\n",
      "require less data and expertise to use; such lower barriers to access could increase both the benefits\n",
      "and associated risks of such models.\n",
      "\n",
      "ENVIRONMENTAL CONSIDERATIONS\n",
      "\n",
      "We use the same pretrained language models as{Austin et al.| (2021). The energy cost and carbon\n",
      "footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional\n",
      "instruction tuning gradient-steps for finetuning FLAN is less than 2% of the number of pretraining\n",
      "steps, and so the estimated additional energy cost is comparatively smaller.\n",
      "\n",
      "AUTHOR CONTRIBUTIONS\n",
      "\n",
      "Maarten Bosma conceived the original idea and implemented the first version of FLAN. Vincent Zhao\n",
      "prototyped the training and evaluation pipelines, as well as rank classification. Kelvin Guu proposed\n",
      "and implemented the idea of task clusters and evaluation using inter-cluster splits. Jason Wei, Maarten\n",
      "Bosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. Jason Wei, Vincent Zhao,\n",
      "and Adams Wei Yu conducted and managed most of the experiments. Jason Wei designed and ran the\n",
      "ablation studies. Jason Wei, Maarten Bosma, and Quoc V. Le wrote most of the paper. Jason Wei,\n",
      "Maarten Bosma, and Nan Du obtained the zero and few-shot baselines. Vincent Zhao and Kelvin Guu\n",
      "designed, implemented, and conducted the few-shot FLAN experiments. Maarten Bosma and Jason\n",
      "Wei ran the data contamination analysis. Brian Lester ran the prompt tuning experiments. Quoc V. Le\n",
      "and Andrew M. Dai advised, provided high-level guidance, and helped edit the paper.\n",
      "\n",
      "ACKNOWLEDGEMENTS\n",
      "\n",
      "We thank Ed Chi, Slav Petrov, Dan Garrette, Ruibo Liu, and Clara Meister for providing feedback\n",
      "on our manuscript. We thank Adam Roberts, Liam Fedus, Hyung Won Chung, and Noam Shazeer\n",
      "for helping debug some of our models. We thank Ellie Pavlick for feedback on the study design\n",
      "during the middle stages of the project. We thank Daniel De Freitas Adiwardana for helping initiate\n",
      "the project, large language model advising, and giving us access to some computational resources.\n",
      "Finally, we thank the team involved in pretraining LaMDA-PT: Daniel De Freitas Adiwardana, Noam\n",
      "Shazeer, Yanping Huang, Dmitry Lepikhin, Dehao Chen, Yuanzhong Xu and Zhifeng Chen.\n",
      "\n",
      "10\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and\n",
      "Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. arXiv preprint\n",
      "\n",
      "arXiv:2101.11038, 2021. URL https: //arxiv.org/abs/2101.11038\n",
      "\n",
      "Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\n",
      "Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine\n",
      "translation in the wild: Findings and challenges. arXiv preprint arXiv: 1907.05019, 2019. URL\n",
      "https: //arxiv.org/abs/1907.05019\n",
      "\n",
      "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\n",
      "Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large\n",
      "\n",
      "language models. arXiv preprint arXiv:2108.07732, 2021. URL: https: //arxiv.org/abs/\n",
      "2108.07732\n",
      "\n",
      "Amittai Axelrod, Xiaodong He, and Jianfeng Gao. Domain adaptation via pseudo in-domain data\n",
      "selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language\n",
      "\n",
      "Processing, pp. 355-362, 2011. URL|https: //aclanthology.org/D11-1033)\n",
      "\n",
      "Marta Bajfion, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espla-Gomis,\n",
      "Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo\n",
      "Pla Sempere, Gema Ramirez-Sanchez, Elsa Sarrias, Marek Strelec, Brian Thompson, William\n",
      "Waites, Dion Wiggins, and Jaume Zaragoza. ParaCrawl: Web-scale acquisition of parallel corpora.\n",
      "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.\n",
      "\n",
      "4555-4567, 2020. URL|https://aclanthology.org/2020.acl-main. 417\n",
      "\n",
      "Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and under-\n",
      "standing in the age of data. In Proceedings of the 58th Annual Meeting of the Association for\n",
      "\n",
      "Computational Linguistics, pp. 5185-5198, 2020. URL https: //aclanthology.org/\n",
      "\n",
      "2020.acl—main. 463\n",
      "\n",
      "Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\n",
      "the dangers of stochastic parrots: Can language models be too big? In Proceedings of the\n",
      "2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT °21, pp. 610-623.\n",
      "\n",
      "Association for Computing Machinery, 2021. URL https: //doi.org/10.1145/3442188 |\n",
      "\n",
      "3445922\n",
      "\n",
      "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The Fifth PASCAL Recognising\n",
      "\n",
      "Textual Entailment Challenge. In TAC, 2009. URL https: //citeseerx. ist psu. edu/|\n",
      "\n",
      "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning\n",
      "about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial\n",
      "\n",
      "Intelligence, 2020. URL https: //arxiv.org/abs/1911.11641\n",
      "\n",
      "Ondiej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz,\n",
      "Matt Post, and Lucia Specia (eds.). Proceedings of the Ninth Workshop on Statistical Machine\n",
      "\n",
      "Translation, 2014. URL https: //aclanthology. org/W14-3300!\n",
      "\n",
      "Ondfej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow,\n",
      "Matthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana Neves, Pavel Pecina, Martin\n",
      "Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, Jorg\n",
      "Tiedemann, and Marco Turchi (eds.). Proceedings of the First Conference on Machine Translation:\n",
      "\n",
      "Volume 1, Research Papers, 2016. URL. https: //aclanthology.org/W16-2200\n",
      "\n",
      "Rishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S.\n",
      "Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, E. Brynjolfsson, S$. Buch, D. Card,\n",
      "Rodrigo Castellon, Niladri S. Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora\n",
      "Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, S. Ermon, J. Etchemendy, Kawin\n",
      "Ethayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Good-\n",
      "man, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho,\n",
      "\n",
      "11\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\n",
      "Siddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay\n",
      "Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Is-\n",
      "abelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P.\n",
      "Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, D. Narayanan, Ben\n",
      "Newman, Allen Nie, J. C. Niebles, H. Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel\n",
      "Papadimitriou, Joon Sung Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan,\n",
      "Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher\n",
      "R’e, D. Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan\n",
      "Taori, Armin W. Thomas, Florian Tramér, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu,\n",
      "Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang,\n",
      "Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On\n",
      "the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL\n",
      "https://arxiv.org/abs/2108.07258\n",
      "\n",
      "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\n",
      "annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\n",
      "on Empirical Methods in Natural Language Processing, pp. 632-642, 2015. URL [https\n",
      "\n",
      "7//aclanthology.org/D15-1075\n",
      "\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n",
      "Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\n",
      "Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\n",
      "\n",
      "Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\n",
      "\n",
      "and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Pro-\n",
      "\n",
      "cessing Systems, volume 33, pp. 1877-1901, 2020. URL|https://proceedings.neurips\n",
      "2020\n",
      "\n",
      "Duo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei Li. Description based text classification with\n",
      "reinforcement learning. In Proceedings of the International Conference on Machine Learning, pp.\n",
      "\n",
      "1371-1382. PMLR, 2020. URL|http://proceedings.mlr.press/v119/chai20a/\n",
      "chai20a.pdf\n",
      "\n",
      "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,\n",
      "Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained\n",
      "\n",
      "on code. arXiv preprint arXiv:2107.03374, 2021. URLihttps://arxiv.org/abs/2107\n",
      "03374\n",
      "\n",
      "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\n",
      "Zettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference\n",
      "on Empirical Methods in Natural Language Processing, pp. 2174-2184, 2018. URL https\n",
      "\n",
      "//aclanthology.org/D18-1241\n",
      "\n",
      "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\n",
      "Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings\n",
      "of the 2019 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Volume I (Long and Short Papers), pp. 2924-2936,\n",
      "\n",
      "2019a. URLihttps://aclanthology.org/N19-1300\n",
      "\n",
      "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le.\n",
      "BAM! born-again multi-task networks for natural language understanding. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Computational Linguistics, pp. 5931-5937, 2019b.\n",
      "\n",
      "URLihttps://aclanthology.org/P19-1595\n",
      "\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n",
      "Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge.\n",
      "arXiv preprint arXiv: 1803.05457, 2018. URLhttps://ar org/abs/1803.05457\n",
      "\n",
      "Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel\n",
      "Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning\n",
      "\n",
      "12\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Research, 12:2493-2537, 2011. URL |https://www.jmlr.org/papers/volumel2/\n",
      "collobertlla/collobertlila.pdf\n",
      "\n",
      "Michele Corazza, Stefano Menini, Elena Cabrio, Sara Tonelli, and Serena Villata. Hybrid emoji-\n",
      "based masked language models for zero-shot abusive language detection. In Findings of the\n",
      "Association for Computational Linguistics: EMNLP 2020, pp. 943-949, 2020. URL https\n",
      "//aclanthology.org/2020.findings-—emnlp. 84\n",
      "\n",
      "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL Recognising Textual Entailment\n",
      "challenge. In Proceedings of the First International Conference on Machine Learning Challenges:\n",
      "Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entail-\n",
      "\n",
      "ment, MLCW’05, pp. 177-190, 2005. URL|https: //doi. org/10.1007/11736790_9\n",
      "\n",
      "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Proceedings of the Confer-\n",
      "\n",
      "ence on Neural Information Processing Systems, 2015. URL|https://papers.nips.cc/\n",
      "paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf\n",
      "\n",
      "Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\n",
      "Investigating projection in naturally occurring discourse. In Proceedings of Sinn und Bedeutung, pp.\n",
      "107-124, 2019. URL https://ojs.ub.uni-konstanz.de/sub/index.php/sub/\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\n",
      "deep bidirectional transformers for language understanding. In Proceedings of the 2019 Con-\n",
      "ference of the North American Chapter of the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume I (Long and Short Papers), pp. 4171-4186, 2019. URL\n",
      "\n",
      "https: //aclanthology.org/N19-1423)\n",
      "\n",
      "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\n",
      "In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\n",
      "\n",
      "https://aclantho ogy .org/105-5002|\n",
      "\n",
      "Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\n",
      "Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language\n",
      "models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021. URL\n",
      "\n",
      "arxiv.org/pdf/2112.06905\n",
      "\n",
      "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\n",
      "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\n",
      "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-\n",
      "putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n",
      "\n",
      "2368-2378, 2019. URL|https://aclanthology.org/N19-1246\n",
      "\n",
      "Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based\n",
      "machine translation systems for WMT-14. In Proceedings of the Ninth Workshop on Statistical\n",
      "\n",
      "Machine Translation, pp. 97-104, 2014. URL|ht tps: //aclanthology .org/W14-3309)\n",
      "\n",
      "Ondiej DuSek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural\n",
      "language generation. In Proceedings of the 12th International Conference on Natural Language\n",
      "\n",
      "Generation, pp. 421-426, 2019. URL https: //aclanthology.org/W19-8652\n",
      "\n",
      "Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale.\n",
      "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\n",
      "\n",
      "pp. 489-500, 2018. URL|https: //aclanthology.org/D18-1045\n",
      "\n",
      "Avia Efrat and Omer Levy. The Turking Test: Can language models understand instructions? arXiv\n",
      "\n",
      "preprint arXiv:2010.11982, 2020. URL https: //arxiv.org/abs/2010.11982\n",
      "\n",
      "Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale\n",
      "multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Computational Linguistics, pp. 1074-1084, 2019. URL\n",
      "\n",
      "https: //aclanthology.org/P19-1102|\n",
      "\n",
      "13\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Fast.AlI. Yelp Sentiment Classification Dataset. https://course.fast.ai/datasets\n",
      "\n",
      "William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\n",
      "models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. URL/https?\n",
      "//arxiv.org/abs/2101.03961\n",
      "\n",
      "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\n",
      "deep networks. In Proceedings of the International Conference on Machine Learning (ICML), pp.\n",
      "\n",
      "1126-1135, 2017. URLihttps://arxiv.org/abs/1703.03400\n",
      "\n",
      "Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\n",
      "learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-\n",
      "tics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\n",
      "\n",
      "Papers), pp. 3816-3830, 2021. URL\n",
      "\n",
      "Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG\n",
      "challenge: Generating text from RDF data. In Proceedings of the 10th International Conference\n",
      "\n",
      "on Natural Language Generation, pp. 124-133, 2017. URL|https: //aclanthology.org/\n",
      "W17-3518\n",
      "\n",
      "Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, An-\n",
      "uoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan\n",
      "Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondfej DuSek, Chris Chinenye Emezue, Varun\n",
      "Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani,\n",
      "Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica\n",
      "Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique\n",
      "Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi\n",
      "Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura\n",
      "Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank San-\n",
      "thanam, Joao Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio\n",
      "Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola,\n",
      "and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics.\n",
      "In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics\n",
      "\n",
      "(GEM 2021), pp. 96-120, 2021. URL|https://aclanthology.org/2021.gem-1.10\n",
      "\n",
      "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\n",
      "textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\n",
      "\n",
      "and Paraphrasing, pp. 1-9, 2007. URL. https: //aclanthology.org/W07-1401\n",
      "\n",
      "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-\n",
      "annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop\n",
      "\n",
      "on New Frontiers in Summarization, pp. 70-79, 2019. URL|https://aclanthology.org/\n",
      "D19-5409\n",
      "\n",
      "Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision.\n",
      "CS224N project report, Stanford, 1(12):2009, 2009. URL https://www-cs.stanford\n",
      "edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
      "\n",
      "Dan Goldwasser and Dan Roth. Learning from natural instructions. | Machine learn-\n",
      "\n",
      "ing, 94(2):205—232, 2014. URL|https://link.springer.com/article/10.1007/\n",
      "\n",
      "Max Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with\n",
      "diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\n",
      "\n",
      "Papers), pp. 708-719, 2018. URL|ht tps: //aclantho logy .org/N18-1065)\n",
      "\n",
      "R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\n",
      "Szpektor. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings\n",
      "of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. URL\n",
      "\n",
      "http://www.cs.biu.ac.il/~szpekti/papers/RTE2-organizers.pdf\n",
      "\n",
      "14\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Luheng He, Mike Lewis, and Luke Zettlemoyer. Question-answer driven semantic role labeling:\n",
      "Using natural language to annotate natural language. In Proceedings of the 2015 Conference\n",
      "on Empirical Methods in Natural Language Processing, pp. 643-653, 2015. URL [https\n",
      "//aclanthology.org/D15-1076\n",
      "\n",
      "Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form\n",
      "competition: Why the highest probability answer isn’t always right. In Proceedings of the\n",
      "2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL https:\n",
      "\n",
      "//aclanthology.org/2021.emnlp-main.564\n",
      "\n",
      "Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. To-\n",
      "ward semantics-based answer pinpointing. In Proceedings of the First International Confer-\n",
      "\n",
      "ence on Human Language Technology Research, 2001. URL|https://www.aclweb.org/\n",
      "anthology/H01-1069|\n",
      "\n",
      "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In\n",
      "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n",
      "\n",
      "1: Long Papers), pp. 328-339, 2018. URL https: //aclanthology. org/P18-1031)\n",
      "\n",
      "Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading\n",
      "comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference\n",
      "on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
      "on Natural Language Processing (EMNLP-IJCNLP), pp. 2391-2401, 2019. URL|https://|\n",
      "\n",
      "aclanthology.org/D19-1243\n",
      "\n",
      "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\n",
      "Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey\n",
      "Dean. Google’s multilingual neural machine translation system: Enabling zero-shot transla-\n",
      "tion. Transactions of the Association for Computational Linguistics, 5:339-351, 2017. URL\n",
      "https://aclanthology.org/Q17-1024\n",
      "\n",
      "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\n",
      "supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601-\n",
      "\n",
      "1611, 2017. URL https: //aclanthology.org/P17-1147\n",
      "\n",
      "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\n",
      "beyond the surface: A challenge set for reading comprehension over multiple sentences. In\n",
      "Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu-\n",
      "tational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252-262, 2018.\n",
      "\n",
      "URL\\https://aclanthology.org/N18-1023\n",
      "\n",
      "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\n",
      "Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In\n",
      "Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896-1907, 2020.\n",
      "\n",
      "URLihttps://aclanthology.org/2020.findings—emnlp.171\n",
      "\n",
      "Dimitrios Kotzias, Misha Denil, Nando de Freitas, and Padhraic Smyth. From group to individual\n",
      "labels using deep features. Proceedings of the 21th ACM SIGKDD International Conference\n",
      "on Knowledge Discovery and Data Mining, 2015. URL {https ://dl.acm.org/doi/10 |\n",
      "\n",
      "1145/2783258.2783380|\n",
      "\n",
      "Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\n",
      "tokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu (eds.),\n",
      "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\n",
      "EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pp.\n",
      "66-71. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-2012. URL\n",
      "https://doi.org/10.18653/v1/d18-2012\n",
      "\n",
      "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Lyyer, James Bradbury, Ishaan Gulrajani, Victor\n",
      "Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for\n",
      "natural language processing. In Proceedings of the International Conference on Machine Learning,\n",
      "\n",
      "pp. 1378-1387. PMLR, 2016. URL https: //arxiv.org/abs/1506.07285\n",
      "\n",
      "15\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\n",
      "Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\n",
      "Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\n",
      "Petrov. Natural Questions: A benchmark for question answering research. Transactions of the\n",
      "\n",
      "Association for Computational Linguistics, 7:452-466, 2019. URL|https://aclanthology\n",
      "org/Q19-1026\n",
      "\n",
      "Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new\n",
      "benchmark dataset for cross-lingual abstractive summarization. In Findings of the Associ-\n",
      "ation for Computational Linguistics: EMNLP 2020, pp. 4034—4048, 2020. URL\n",
      "\n",
      "//aclanthology.org/2020.findings-emnlp. 360)\n",
      "\n",
      "Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object\n",
      "classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and\n",
      "\n",
      "Pattern Recognition, pp. 951-958. IEEE, 2009. URL\n",
      "document /5206594\n",
      "\n",
      "Anne Lauscher, Vinit Ravishankar, Ivan Vuli¢, and Goran Glava8. From zero to hero: On the\n",
      "limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the\n",
      "2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)), pp. 4483-4499,\n",
      "\n",
      "2020. URLihttps://aclanthology.org/2020.emnlp-main. 363\n",
      "\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\n",
      "open domain question answering. In Proceedings of the 57th Annual Meeting of the Association\n",
      "\n",
      "for Computational Linguistics, pp. 6086-6096, 2019. URL|https://aclanthology.org/\n",
      "\n",
      "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\n",
      "Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\n",
      "computation and automatic sharding. In /nternational Conference on Learning Representations,\n",
      "\n",
      "2020. URL|https://openreview.net/forum?id=grwe7XHTmYb\n",
      "\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,\n",
      "\n",
      "2021. URLjhttps://arxiv.org/abs/2104.08691\n",
      "\n",
      "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In\n",
      "Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning,\n",
      "\n",
      "2012. URL|https://dl.acm.org/doi/10.5555/3031843.3031909\n",
      "\n",
      "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\n",
      "via reading comprehension. In Proceedings of the 21st Conference on Computational Natural\n",
      "\n",
      "Language Learning (CoNLL 2017), pp. 333-342, 2017. URL /https://aclanthology|\n",
      "\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\n",
      "Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and comprehension. In Proceedings of the 58th\n",
      "Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, 2020. URL\n",
      "https://aclanthology.org/2020.acl-main.703\n",
      "\n",
      "Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\n",
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n",
      "11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\n",
      "\n",
      "pp. 4582-4597, 2021. URL|https://aclanthology.org/2021.acl-long. 353|\n",
      "\n",
      "Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. A unified\n",
      "MRC framework for named entity recognition. In Proceedings of the 58th Annual Meet-\n",
      "ing of the Association for Computational Linguistics, pp. 5849-5859, 2020. URL\n",
      "\n",
      "//aclanthology.org/2020.acl-—main.519\n",
      "\n",
      "16\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Confer-\n",
      "ence on Computational Linguistics, 2002. URL|https ://www.aclweb. org/anthology/\n",
      "CO2-1150\n",
      "\n",
      "Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and\n",
      "Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense\n",
      "reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp.\n",
      "\n",
      "1823-1840, 2020. URL/https://aclanthology.org/2020.findings-—emnlp. 165}\n",
      "\n",
      "Han Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai Li, Xiao-Ming Wu, and Albert Y.S. Lam.\n",
      "Reconstructing capsule networks for zero-shot intent classification. In Proceedings of the 2019\n",
      "Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\n",
      "Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4799-4809, 2019a. URL\n",
      "\n",
      "https: //aclanthology.org/D19-1486)\n",
      "\n",
      "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhenbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\n",
      "prompt, and predict: A systematic survey of prompting methods in natural language processing.\n",
      "\n",
      "arXiv preprint arXiv:2107.13586, 2021. URL https: //arxiv.org/abs/2107.13586\n",
      "\n",
      "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\n",
      "natural language understanding. In Proceedings of the 57th Annual Meeting of the Association\n",
      "\n",
      "for Computational Linguistics, pp. 4487-4496, 2019b. URL https: //aclanthology.org/\n",
      "P19- 441)\n",
      "\n",
      "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike\n",
      "Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation.\n",
      "Transactions of the Association for Computational Linguistics, 8:726-742, 2020. URL|https ]\n",
      "\n",
      "//aclanthology.org/2020.tacl-1.47\n",
      "Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "\n",
      "sequence to sequence learning. Proceedings of ICLR, 2016. URL https://arxiv.org/\n",
      "abs/1511.06114\n",
      "\n",
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\n",
      "Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\n",
      "of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,\n",
      "Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL |http/\n",
      "\n",
      "//www.aclweb.org/anthology/P11-1015\n",
      "\n",
      "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\n",
      "decathlon: Multitask learning as question answering. arXiv preprint arXiv: 1806.08730, 2018.\n",
      "\n",
      "URLihttps://arxiv.org/abs/1806.08730\n",
      "\n",
      "John McCarthy. Programs with common sense. RLE and MIT computation center, 1960. URL\n",
      "http://jmc.stanford.edu/articles/mcc59/mcc59.pdf\n",
      "\n",
      "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\n",
      "electricity? A new dataset for open book question answering. In Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Language Processing, pp. 2381-2391, 2018. URL\n",
      "https://aclanthology.org/D18-1260\n",
      "\n",
      "Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn\n",
      "\n",
      "in context. arXiv preprint arXiv:2110.15943, 2021. URL/https://arxiv.org/abs/2110\n",
      "\n",
      "15943\n",
      "\n",
      "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Natural Instructions:\n",
      "Benchmarking generalization to new tasks from natural language instructions. arXiv preprint\n",
      "\n",
      "arXiv:2104.08773, 2021. URL https: //arxiv.org/abs/2104.08773\n",
      "\n",
      "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vander-\n",
      "wende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding\n",
      "of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Technologies, pp. 839-849,\n",
      "\n",
      "2016. URL https: //aclanthology.org/N16-1098\n",
      "\n",
      "17\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\n",
      "Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto,\n",
      "Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao\n",
      "Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani.\n",
      "DART: Open-domain structured data record to text generation. In Proceedings of the 2021\n",
      "Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "\n",
      "Human Language Technologies, pp. 432-447, 2021. URL\n",
      "2021.naacl-main. 37)\n",
      "\n",
      "Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. Annotated Gigaword. In Pro-\n",
      "ceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale\n",
      "Knowledge Extraction (AKBC-WEKEX), pp. 95-100, 2012. URL|https: //aclanthology}\n",
      "\n",
      "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary!\n",
      "topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Language Processing, pp. 1797-1807, 2018. URL\n",
      "\n",
      "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\n",
      "NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Linguistics, pp. 4885-4901, 2020. URL|https:\n",
      "//aclanthology.org/2020.acl-main. 441\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\n",
      "Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\n",
      "Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\n",
      "Ryan Lowe. Training language models to follow instructions with human feedback. Preprint,\n",
      "2022. URL\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit lyyer, Matt Gardner, Christopher Clark, Kenton Lee,\n",
      "and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018\n",
      "Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, 2018. URL|https:\n",
      "//aclanthology.org/N18-1202\n",
      "\n",
      "Ngoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and Alexander Waibel. Improving zero-shot translation\n",
      "with language-independent constraints. In Proceedings of the Fourth Conference on Machine\n",
      "\n",
      "Translation (Volume 1: Research Papers), pp. 13-23, 2019. URL|https://aclanthology}\n",
      "\n",
      "Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for\n",
      "evaluating context-sensitive meaning representations. In Proceedings of the 2019 Confer-\n",
      "ence of the North American Chapter of the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume I (Long and Short Papers), pp. 1267-1273, 2019. URL\n",
      "https: //aclanthology.org/N19-1128)\n",
      "\n",
      "Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Hannun, Vitaliy Liptchinsky, Gabriel\n",
      "Synnaeve, and Ronan Collobert. Massively multilingual ASR: 50 languages, 1 model, | billion\n",
      "\n",
      "parameters. arXiv preprint arXiv:2007.03001, 2020. URL/https://arxiv.org/abs/2007\n",
      "\n",
      "03001\n",
      "\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts.\n",
      "In Proceedings of the 2021 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 5203-5212, 2021.\n",
      "\n",
      "URLihttp://cs.jhu.edu/~jason/papers/#qin-eisner-2021\n",
      "\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\n",
      "language understanding by generative pre-training. https://blog.openai.com/\n",
      "\n",
      "language-unsupervised, 2018.\n",
      "\n",
      "18\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n",
      "Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. URL\n",
      "https: //d4mucfpksywv.cloudfront.net/better-language-models/\n",
      "language_models_are_unsupervised_multitask_learners.pdf|\n",
      "\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\n",
      "Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\n",
      "text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL\n",
      "\n",
      "http://jmlr.org/papers/v21/20-074. html}\n",
      "\n",
      "Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: The Winograd\n",
      "schema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural\n",
      "Language Processing and Computational Natural Language Learning, pp. 777-789, 2012. URL\n",
      "\n",
      "https: //aclanthology.org/D12-1071)\n",
      "\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQUAD: 100,000+ questions for\n",
      "machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in\n",
      "Natural Language Processing, pp. 2383-2392, 2016. URL|https: //aclanthology.org/|\n",
      "D16-126 4)\n",
      "\n",
      "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\n",
      "for SQUAD. In Proceedings of the 56th Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers), pp. 784-789, 2018. URL|https://aclanthology|\n",
      "org/P18-2124\n",
      "\n",
      "Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering\n",
      "challenge. Transactions of the Association for Computational Linguistics, 7:249-266, 2019. URL\n",
      "\n",
      "https: //aclanthology.org/Q19-1016\n",
      "\n",
      "Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. A\n",
      "recipe for arbitrary text style transfer with large language models. arXiv preprint arXiv:2109.03910,\n",
      "\n",
      "2021. URLhttps://arxiv.org/abs/2109.03910\n",
      "\n",
      "Melissa Roemmele, Cosmin Bejan, and Andrew Gordon. Choice of plausible alternatives: An\n",
      "evaluation of commonsense causal reasoning. In AAAI Spring Symposium Series, 2011. URL\n",
      "\n",
      "https: //www.aaai .org/ocs/index.php/SSS/SSS11/paper/view/2418,\n",
      "\n",
      "Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot\n",
      "learning. In Proceedings of the International Conference on Machine Learning, pp. 2152-2161,\n",
      "\n",
      "2015. URL|https://proceedings.mlr.press/v37/romera-paredes15.pdf\n",
      "\n",
      "Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\n",
      "\n",
      "arXiv:1706.05098, 2017. URL https: //arxiv.org/abs/1706.05098\n",
      "\n",
      "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adver-\n",
      "sarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial\n",
      "\n",
      "Intelligence, pp. 8732-8740, 2020. URL|https://arxiv.org/abs/1907.10641\n",
      "\n",
      "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\n",
      "Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\n",
      "enables zero-shot task generalization. Proceedings of the International Conference on Learning\n",
      "\n",
      "Representations, 2021. URL https: //arxiv.org/abs/2110.08207\n",
      "\n",
      "David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\n",
      "reasoning abilities of neural models. Proceedings of the International Conference on Learning\n",
      "\n",
      "Representations, 2019. URL https: //arxiv.org/pdf/1904.01557\n",
      "\n",
      "Timo Schick and Hinrich Schiitze. Exploiting cloze-questions for few-shot text classification and\n",
      "natural language inference. In Proceedings of the 16th Conference of the European Chapter\n",
      "of the Association for Computational Linguistics: Main Volume, pp. 255-269, 2021. URL\n",
      "\n",
      "https://aclanthology.org/2021.eacl-main.20\n",
      "\n",
      "19\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\n",
      "pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers), pp. 1073-1083, 2017. URL|https://|\n",
      "aclanthology.org/P17-1099\n",
      "\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems\n",
      "for WMT 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared\n",
      "\n",
      "Task Papers, pp. 371-376, 2016. URL|https: //aclanthology .org/W16=2323\n",
      "\n",
      "Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\n",
      "cost. In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018. URL\n",
      "\n",
      "https: / /arxiv.org/abs/1804.04235)\n",
      "\n",
      "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and\n",
      "Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\n",
      "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,\n",
      "\n",
      "pp. 1631-1642, 2013. URL/https://aclanthology.org/D13-1170\n",
      "\n",
      "Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with\n",
      "\n",
      "values-targeted datasets. arXiv preprint arXiv:2106.10328, 2021. URL\n",
      "abs/2106 -10328\n",
      "\n",
      "Shashank Srivastava, Igor Labutov, and Tom Mitchell. Zero-shot learning of classifiers from natural\n",
      "language quantification. In Proceedings of the 56th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers), pp. 306-316, 2018. URL |https://\n",
      "aclanthology.org/P18-1029\n",
      "\n",
      "Derek Tam, Menton Rakesh R., Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving\n",
      "and simplifying pattern exploiting training. In Proceedings of the 2021 Conference on Empirical\n",
      "\n",
      "Methods in Natural Language Processing (EMNLP), 2021. URL/https://arxiv.org/pdf/\n",
      "2103.11955\n",
      "\n",
      "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\n",
      "Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog\n",
      "\n",
      "applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/pdf/\n",
      "2201.08239\n",
      "\n",
      "Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018. URL\n",
      "\n",
      "https: / /arxiv.org/abs/1810.03548)\n",
      "\n",
      "Marc Velay and Fabrice Daniel. Seq2seq and multi-task learning for joint intent and content\n",
      "extraction for domain specific interpreters. arXiv preprint arXiv: 1808.00423, 2018. URL|https?\n",
      "\n",
      "//arxiv.org/abs/1808.00423\n",
      "\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\n",
      "A multi-task benchmark and analysis platform for natural language understanding. In Proceedings\n",
      "of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\n",
      "NLP, pp. 353-355, 2018. URL|https://aclanthology.org/W1\n",
      "\n",
      "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\n",
      "Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language\n",
      "understanding systems. Conference on Neural Information Processing Systems (NeurIPS), 2019a.\n",
      "\n",
      "URLihttps://arxiv.org/abs/1905.00537\n",
      "\n",
      "Lu Wang and Wang Ling. Neural network-based abstract generation for opinions and arguments.\n",
      "In Proceedings of the 2016 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies, pp. 47-57, 2016. URL https:\n",
      "//www.aclweb.org/anthology/N16-1007\n",
      "\n",
      "Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-\n",
      "agent dual learning. In Proceedings of the International Conference on Learning Representations\n",
      "\n",
      "(ICLR) 2019, 2019b. URL https: //openreview.net/forum? id=HyGhN2A5tm|\n",
      "\n",
      "20\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Zirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. Towards zero-label language learning. arXiv\n",
      "preprint arXiv:2109.09193, 2021. URL. https: //arxiv.org/abs/2109. 09193)\n",
      "\n",
      "Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.\n",
      "Transactions of the Association for Computational Linguistics, 7:625—641, 2019. doi: 10.1162/\n",
      "\n",
      "tacl_a_00290. URL|https://aclanthology.org/Q19-1040\n",
      "\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\n",
      "Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\n",
      "\n",
      "arXiv:2201.11903, 2022. URL https: //arxiv.org/pdf/2201.11903\n",
      "\n",
      "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\n",
      "tence understanding through inference. In Proceedings of the 2018 Conference of the North Ameri-\n",
      "can Chapter of the Association for Computational Linguistics: Human Language Technologies,\n",
      "\n",
      "Volume 1 (Long Papers), pp. 1112-1122, 2018. URL/http://aclweb.org/anthology/\n",
      "\n",
      "Joseph Worsham and J. Kalita. Multi-task learning for natural language processing in the 2020s:\n",
      "\n",
      "where are we going? arXiv preprint arXiv:2007.16008, 2020. URL|https://arxiv.org/|\n",
      "\n",
      "Jeff Wu, Long Ouyang, Daniel M Ziegler, Nissan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris-\n",
      "tiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862,\n",
      "\n",
      "2021. URL/http /arxiv.org/abs/2109.10862\n",
      "\n",
      "Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei Li. CorefQA: Coreference resolution as\n",
      "query-based span prediction. In Proceedings of the 58th Annual Meeting of the Association\n",
      "\n",
      "for Computational Linguistics, pp. 6953-6963, 2020. URL https: //aclanthology.org/\n",
      "\n",
      "2020.acl—main. 622\n",
      "\n",
      "Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task\n",
      "generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural\n",
      "\n",
      "Language Processing (EMNLP), 2021. URL|hnttps://arxiv.org/abs/2104.08835\n",
      "\n",
      "Wenpeng Yin, Jamaal Hay, and Dan Roth. Benchmarking zero-shot text classification: Datasets,\n",
      "evaluation and entailment approach. In Proceedings of the 2019 Conference on Empirical Methods\n",
      "in Natural Language Processing and the 9th International Joint Conference on Natural Language\n",
      "\n",
      "Processing (EMNLP-LICNLP), pp. 3914-3923, 2019. URL\n",
      "\n",
      "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\n",
      "machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\n",
      "\n",
      "for Computational Linguistics, pp. 4791-4800, 2019. URL https: //aclanthology.org/\n",
      "\n",
      "Rui Zhang and Joel Tetreault. This email could save your life: Introducing the task of email subject\n",
      "line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational\n",
      "\n",
      "Linguistics, 2019. URL https: //aclanthology.org/P19-1043\n",
      "\n",
      "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\n",
      "Record: Bridging the gap between human and machine commonsense reading comprehension.\n",
      "\n",
      "CoRR, abs/1810.12885, 2018. URL http: //arxiv.org/abs/1810.12885\n",
      "\n",
      "Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-\n",
      "\n",
      "sification. In NJPS, 2015. URL|https://proceedings.neurips.cc/paper/2015/\n",
      "file/250cf8b51c773£3f8dc8b4be867a9a02—-Paper.pdf\n",
      "\n",
      "Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\n",
      "\n",
      "pp. 1298-1308, 2019. URLjhttps://aclanthology.org/N19-1131\n",
      "\n",
      "Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Meta-tuning language models to answer\n",
      "\n",
      "prompts better. arXiv preprint arXiv:2104.04670, 2021. URL|https://arxiv.org/abs/\n",
      "\n",
      "21\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "A ADDITIONAL RESULTS\n",
      "\n",
      "This section shows the full results for all datasets we evaluate. Results for translation and struct to\n",
      "text are shown in Table[I] and the results for eight NLU task clusters are shown in Table[2|\n",
      "\n",
      "We show FLAN’s performance using the best of up to ten instruction templates as well as the template\n",
      "\n",
      "with the best performance on the dev set. For LaMDA-PT, we use the templates from[Brown et al.]\n",
      "0), which were optimized for GPT-3, without performing any prompt engineering to optimize\n",
      "\n",
      "them on our model. For simplicity, we use greedy search for all generative tasks (compared with\n",
      "\n",
      "beam search used in (2020)). Unlike GPT-3, which chooses the number of few-shot\n",
      "\n",
      "exemplars k via best dev set performance, for few-shot LaMDA-PT we choose the highest & that fits\n",
      "\n",
      "in the context length of 1024 tokens, from k € {1,3,5, 10}.\n",
      "\n",
      "For DROP (Dua et al.|/2019) and SQuADv2 (Rajpurkar et al-|[2018), based on email correspondence\n",
      "\n",
      "with their definition of zero-shot differs from ours in that they actually use\n",
      "exemplars, but only from the same passage as the inference question (each passage has more than\n",
      "one question). Hence, GPT-3 zero-shot results are not directly comparable with ours for DROP and\n",
      "SQuADv2. We mark these results using the * symbol. Moreover, it is unclear how to parse the end of\n",
      "an answer for these two datasets, and so we use curly bracket delimiters { and }, where we expect }\n",
      "to indicate the end of the answer.\n",
      "\n",
      "For struct to text, reported T5/mT5 results are from the GEM benchmark paper\n",
      "(2021), though we do not report their results for DART (through correspondence with authors, we\n",
      "confirmed that their results for DART were incorrect). Though we use a summarization task cluster\n",
      "during instruction tuning, we leave evaluation of summarization for future work, as the mean input of\n",
      "most summarization datasets exceeds FLAN’s input length of 1024 tokens.\n",
      "\n",
      "FLAN 137B\n",
      "LaMDA-PT GPT-3 175B zero-shot few-shot\n",
      "Metric Supervised zero- few- zero- few- average bestdev average best dev 1 Ht\n",
      "Model shot shot (k] shot shot [x] template template template template\n",
      "TRANSLATION\n",
      "WMT ’14 En-+Fr BLEU 35.0% 11.2 31.515) 25.2 32.6 (o4] 32.9411 33.9 33.9402 33.8 19) 5\n",
      "WMT ’14 Fr>En BLEU 45.6° 7.2 34.7 {5} 21.2 39.2 (64) 35.5413 35.9 38.0401 37.9 [9]} 3\n",
      "WMT ’16 En—De BLEU 38.6 7.7 26.7 [3] 24.6 29.7 (64) 25.4418 27.0 26.8104 26.1 [1 5\n",
      "WMT ’16 De->En BLEU 41.2° 20.8 36.8 (5) 27.2 40.6 (64) 38.9403 38.9 40.6401 40.7 [1 3\n",
      "WMT ’16 En—Ro BLEU 39.99 3.5 22.915) 14.1 21.0 [64] 16.7416 18.9 20.5401 20.5 19) 5\n",
      "WMT ’16 Ro-En BLEU 38.59 9.7 37.515) 19.9 39.5 [64] 36.8405 37.3 38.2401 38.1 19) 3\n",
      "STRUCT TO TEXT\n",
      "CommonGen Rouge-1 64.0% 3.9 56.713) — = 54.6423 56.3 56.6403 56.4 [16] 6\n",
      "Rouge-2 29.4% 1.5 29.613) — = 28.8124 27.6 30.9407 29.9 [16 6\n",
      "Rouge-L 54.5° 3.2 48.5.3) - = 48.4419 48.7 50.7402 51.0 [16] 6\n",
      "DART Rouge-1 = 11.3 56.0(3) - = 45.5442 48.9 57.9416 59.2 fi 7\n",
      "Rouge-2 = 1.5 29.6.3) - = 25.0437 30.0 35.8410 36.2 [1 7\n",
      "Rouge-L = 3.2 48.5) - = 38.4438 43.4 48.5409 48.2 [1 7\n",
      "E2ENLG Rouge-1 72.6% 6.2 56.713) — = 44.8439 51.4 59.1413 59.7 p12) 9\n",
      "Rouge-2 47.5° 2.5 314) - = 24.2436 30.1 33.2411 33.6 [12] 9\n",
      "Rouge-L 564° 4.9 41.13) - = 37.0435 42.4 44.9408 45.1 [12] 9\n",
      "WebNLG Rouge-1 83.5% 13.9 68.313) — = 50.6447 57.7 68.5422 71.2 [10) 8\n",
      "Rouge-2 63.6° 6.9 46.03) —- = 29.8442 35.4 48.0415 49.8 [10) 8\n",
      "Rouge-L 71.0% 11.8 56.513) —- = 43.4445 49.7 58.8411 60.2 10) 8\n",
      "Table 1: Results for translation and struct-to-text tasks. [k] indicates the number of few-shot\n",
      "exemplars. # indicates the number of templates that FLAN is evaluated on.“TS-11B, {Edunov et al.\n",
      "018), {Durrani et al.|{2014), {Wang et al.|(2019b), ‘[Sennrich et al.|(2016), {Liu et al.|(2020).\n",
      "\n",
      "22\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "FLAN 137B\n",
      "GLaM LaMDA-PT_ GPT-3 175B zero-shot few-shot\n",
      "\n",
      "Random Supervised Zero- one- zero- few-  zero- few- average best dev average _ best dev\n",
      "\n",
      "Guess Model shot shot shot shot [x] shot shot [&] template template template template !* #t\n",
      "NLI\n",
      "ANLIR1 33.3 57.4° 40.9 42.4 39.6 39.015) 34.6 36.8 150) 47.7414 46.4 44.2423 47.9 [61 8\n",
      "ANLIR2 33.3. 48.3 38.2 40.0 39.9 37.515) 35.4 34.0150) 43.9413 44.0 41.6414 41.1 16 8\n",
      "ANLIR3 33.3. 43.5\" 40.9 40.8 39.3 40.715) 34.5 40.2 [501 47.0413 48.5 42.8422 46.8 16) 8\n",
      "CB 33.3 93.6% 33.9 73.2 42.9 34.4 (5; 46.4 82.1 (32) 64.14147 83.9 82.6444 82.1 [71 10\n",
      "MNLI-m 33.3. 92.2% -— — 35.7 43.715) = - 51.1462 61.2 60.8437 63.5 [10] 10\n",
      "MNLI-mm 33.3 91.9% -— -— 37.0 43.8155) = - 51.0465 62.4 61.0435 63.5 [10] 10\n",
      "QNLI 50.0 96.9% -— — 50.6 55.7 [5] = - 59.6449 66.4 62.0417 63.3 [12] 9\n",
      "RTE 50.0 92.5° 68.8 71.5 73.3 70.815} 63.5 72.9 [32] 78.3479 84.1 79.9469 84.5 [8] 10\n",
      "SNLI 33.3 91.3 - -— 33.3 54.715) = - 43.0474 53.4 62.3424 65.6 [15] 9\n",
      "WNLI 50.0 945° -— — 56.3 64.8 [5] = - 61.04106 74.6 55.4110 70.4 [14] 10\n",
      "READING COMP.\n",
      "BoolQ 50.0 91.2% 83.0 82.8 81.0 80.011) 60.5 77.5 (32) 80.2431 82.9 83.6408 84.6 [41 9\n",
      "DROP = 80.5° 54.9 55.2 3.8 10.3 (1 23.6! 36.5 (20) 21.9409 22.7 22.3411 23.9 (2) 7\n",
      "MultiRC - 88.1% 45.1 62.0 60.0 59.6 {5} 72.9 74.8 (32) 74.5437 77.5 69.2432 72.1 \"1 8\n",
      "OBQA 25.0 85.4\" 53.0 55.2 41.8 50.6 (10) 57.6 65.4 [100}77.4413 78.4 77.2413 78.2 [16] 7\n",
      "SQuADv1 = 96.27 -— — 22.7 50.2 13) = - 79.5+16 80.1 82.li05 82.7 41 8\n",
      "SQuADv2 = 83.4° 68.3 70.0 11.1 34.9 BI 59.51 69.8 {16} 40.9418 44.2 40.8409 43.1 [31 10\n",
      "CLOSED-BOOK QA\n",
      "ARC-c 25.0 81.1% 48.2 50.3 42.0 49.410) 51.4 51.5150) 61.7414 63.1 63.7406 63.8 [13] 7\n",
      "ARC-e 25.0 92.6% 71.9 76.6 76.4 80.9 {10} 68.8 70.1 {50} 79.5408 79.6 80.5405 80.7 [14] 7\n",
      "NQ = 36.67 21.5 23.9 3.2 22.1 [5] 14.6 29.9 (64) 18.6427 20.7 27.2405 27.6 [16] 10\n",
      "TQA (wiki) - 60.5% 68.8 71.5 21.9 63.3 [10) 64.3 71.2 (64) 66.5426 68.1 66.5410 67.3 [16] 10\n",
      "TQA (tfds-devy) = 510° - - 184 55.1,0 - - - 55.0423 56.7 57.2406 57.8 [16] 10\n",
      "COMMONSENSE\n",
      "COPA 50.0 94.8% 90.0 92.0 90.0 89.0 [10} 91.0 92.0 (32) 90.6420 91.0 88.5438 87.0 [16] 8\n",
      "HellaSwag 25.0 47.3° 77.1 76.8 57.0 58.8 io) 78.9 79.3 (20) 56.4405 56.7 59.4402 59.2 [3] 8\n",
      "PIQA 50.0 66.8° 80.4 81.480.3* 80.2* f10) 81.0 82.3 {50} 80.9*+08 80.5* 82.1*+03 81.7% [io] 8\n",
      "StoryCloze 50.0 89.2° 82.5 84.0 79.5 83.7 fio) 83.2 87.7 {70} 92.2413 93.4 93.3409 94.7 [10] 8\n",
      "SENTIMENT\n",
      "IMDB 50.0 95.5% -— —- 76.9 83.31 = - 94.1404 94.3 94.8403 95.0 (21 7\n",
      "Sent140 50.0 87.0° - - 414 63.3 [5] = - 69.9425 73.5 68.7412 69.3 [16] 6\n",
      "SST-2 50.0 97.5° -— — 51.0 92.315) 71.6 95.618} 92.6417 94.6 94.4408 94.6 [16] 8\n",
      "Yelp 50.0 98.1% - - 84.7 89.6131 = - 97.8402 98.1 97.9401 98.0 [41 7\n",
      "PARAPHRASE\n",
      "MRPC 50.0 90.4° -— — 53.7 64.015) = - 69.1413 69.1 67.5417 67.2 [10] 10\n",
      "QQP 50.0 90.6% -— — 34.9 58.9 [3] = - 72.1468 75.9 73.5429 75.9 [16] 7\n",
      "PAWS Wiki 50.0 91.9% -— -— 45.5 53.5 [5] = - 61.5465 69.4 66.2409 70.2 [10) 10\n",
      "COREFERENCE\n",
      "DPR 50.0 848° - - 54.6 57.315) = - 60.3435 66.8 62.4416 63.3 [16] 10\n",
      "Winogrande 50.0 65.8° 73.4 73.0 68.3 68.4 io) 70.2 77.7 {so} 67.3425 71.2 72.3409 72.8 [16] 10\n",
      "WSC273 50.0 70.0° 86.8 83.9 81.0 61.5 [5] 88.3 88.5 [32] 80.843.7 = -+4- -—  [-] 10\n",
      "READ. COMP. W/ COMMONSENSE\n",
      "CosmosQA 25.0 671° -— = 34.1 33.8 [5] = - 58.4413 60.6 56.7413 56.0 {5} 8\n",
      "ReCoRD - 93.4% 90.3 90.3 87.8\" 87.6% 11) 90.2 89.0 [32] 67.8*+3.0 72.5* 77.0* +20 79.0* (1) 10\n",
      "\n",
      "Table 2: Results for eight NLU task clusters. All values shown are for accuracy (or exact match)\n",
      "except DROP, MultiRC, and SQUAD v1 and v2, which are F1. {x] indicates the number of few-shot\n",
      "exemplars. #1 indicates the number of templates that FLAN is evaluated on.?T5-11B, °BERT-large.\n",
      "does not have training or\n",
      "validation sets, and so we do not compute few-shot results for FLAN. For Trivia QA (TQA), we\n",
      "report exact match (EM) on both the wikipedia subset of the dev set to compare with GPT-3, as well\n",
      "\n",
      "*see data contamination (Appendix[C). WSC273\n",
      "\n",
      "as the full TFDS dev set.\n",
      "\n",
      "23\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "B FURTHER ABLATION STUDIES AND ANALYSIS\n",
      "\n",
      "B.1 DATASETS PER TASK CLUSTER & TEMPLATES PER DATASET\n",
      "\n",
      "Our primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on\n",
      "unseen tasks. showed that adding more task clusters improves performance; here, we further\n",
      "explore whether adding additional datasets improves performance when the number of task clusters\n",
      "is held constant. We use the same split as in where the NLI, commonsense reasoning, and\n",
      "closed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning. For\n",
      "these seven task clusters, we instruction tune models using just one dataset per task cluster and using\n",
      "four datasets per task cluster (for task clusters that did not have four tasks, we just used all available\n",
      "tasks). In addition, we simultaneously explore the role of the number of instruction templates per\n",
      "dataset; as mentioned in for each dataset we manually composed ten instructional templates for\n",
      "instruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset.\n",
      "\n",
      "Figure[I1]shows these results. Using more datasets per cluster improved performance by almost\n",
      "10% on average across the three held-out clusters. Using more templates per dataset, however,\n",
      "had a comparatively negligible effect on performance when there was one task per cluster, which\n",
      "disappeared when there were four tasks per cluster. The small effect of templates is striking given our\n",
      "original motivation that composing ten templates per task would mitigate overfitting to any particular\n",
      "template. This results serves to underscore, however, the unpredictability of finetuning large language\n",
      "models, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task.\n",
      "\n",
      "7% A Datasets\n",
      "\n",
      "per task\n",
      "\n",
      "70.1 70.5 69.9 cluster = 1\n",
      "\n",
      "70 @ Datasets\n",
      "per task\n",
      "cluster = 4\n",
      "\n",
      "Performance\n",
      "\n",
      "2 4 6 8 10\n",
      "Templates per dataset\n",
      "\n",
      "Figure 11: Effect of datasets per task cluster and templates per dataset on performance on three\n",
      "held-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per task\n",
      "cluster substantially improves performance. Using more templates per dataset, however, only had\n",
      "a very small effect on performance, which disappeared when there were sufficient dataset per task\n",
      "cluster.\n",
      "\n",
      "B.2 ROLE OF INSTRUCTIONS DURING FINETUNING\n",
      "\n",
      "The per-cluster results for the ablation study from\n",
      "\n",
      "are shown in Table[3]\n",
      "\n",
      "B.3. FURTHER ANALYSIS: INSTRUCTION TUNING FACILITATES PROMPT TUNING\n",
      "\n",
      "The per-dataset results for the analysis in §4.5]are given in Table[4] As the above tasks are all\n",
      "classification, further work in this direction might include tasks such as summarization or question\n",
      "answering, or try to finetune the model using the supervised datasets.\n",
      "\n",
      "C DATA CONTAMINATION ANALYSIS\n",
      "\n",
      "One reasonable concern is that since the pretraining corpus of FLAN has more than 2 trillion tokens,\n",
      "it is possible that examples from a given evaluation dataset may have already been seen verbatim\n",
      "by the model during pre-training, hence inflating the performance of our purported zero-shot model.\n",
      "To this end, like GPT-3 (Brown et al.| 0), we perform post-hoc data contamination analysis to\n",
      "\n",
      "24\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Zero-shot performance on unseen task cluster\n",
      "\n",
      "7 + Read. —Closed- . Four-Task\n",
      "Finetuning prompt Inference prompt NLI Comp. Book QA Translation ‘Average\n",
      "Natural instructions Natural instructi 56.2 77.4 56.6 30.7 55.2\n",
      "(= FLAN) atural instructions . . . . .\n",
      "No template Natural instructions 50.5 58.2 25.5 15.0 37.3\n",
      "Task/dataset name Natural instructions 52.8 63.0 44.8 25.9 46.6\n",
      "Task/dataset name Task/datasetname 60.2 64.9 40.8 21.9 47.0\n",
      "\n",
      "Table 3: Ablation study result using models where instructions are removed from the finetuning\n",
      "process. In “no template,” only inputs and outputs are given, which does not distinguish among\n",
      "tasks during multi-task finetuning. In “task/dataset name”, inputs during multi-task finetuning are\n",
      "prepended with the name of the task and dataset (e.g., “/Translation: WMT’ 14 to French] The dog\n",
      "runs”) NLI datasets: ANLI R1-R3, CB, and RTE; reading comprehension datasets: BoolQ, MultiRC,\n",
      "and OpenbookQA; closed-book QA datasets: ARC-c, ARC-e, NQ, and TQA; translation datasets:\n",
      "WMT’ 14 Frss+En, WMT’ 16 De++En, and WMT’ 16 RosEn. Notably, training with task/dataset\n",
      "name achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, for\n",
      "which the validation set only has 56 examples (FLAN also gets 83.9 with the best dev template, but\n",
      "the average template was only 64.1).\n",
      "\n",
      "PROMPT TUNING ANALYSIS\n",
      "Prompt tuning BoolQ CB CoPA MultiRC ReCoRD RTE WiC WSC\n",
      "\n",
      "train. examples acc. acc. acc. FI acc. acc. acc. — ace.\n",
      "LaMDA-PT 32 55.5 55.4 87.0 65.4 78.0 52.4 516 65.4\n",
      "FLAN 775 87.5 91.0 76.8 80.8 83.0 57.8 70.2\n",
      "LaMDA-PT full 82.8 87.5 90.0 78.6 84.8 82.0 54.9 72.7\n",
      "FLAN dataset 86.3 98.2 94.0 83.4 85.1 91.7 74.0 86.5\n",
      "\n",
      "Table 4: FLAN (instruction tuning) responds better to continuous inputs attained via prompt tuning\n",
      "than LaMDA-PT (no instruction tuning). When prompt tuning on a given dataset, no tasks from the\n",
      "same cluster as that dataset were seen during instruction tuning.\n",
      "\n",
      "investigate whether the performance of the model is in fact inflated by evaluating on examples that\n",
      "occurred in the pretraining dataset.\n",
      "\n",
      "Our data contamination procedure follows the setup of|Brown et al.|(2020), which, for each bench-\n",
      "mark, produces a “clean” version that removes all potentially leaked examples, defined as examples\n",
      "for which any n-gram (n varies per dataset but is roughly 13) overlapped with anything in the\n",
      "pretraining corpus. We use the same n per dataset as[Brown et al.|(2020) and also split on spaces. We\n",
      "then evaluate our model on this clean subset, comparing against model performance on the original\n",
      "dataset (clean + dirty). Lower performance on the clean subset would suggest that data contamination\n",
      "leads to inflated results.\n",
      "\n",
      "Figure|12)/summarizes these results, with the exact numbers given in Table|5| We see several trends\n",
      "very similar to those in the GPT-3 paper: (1) many datasets had a substantial number of examples\n",
      "that overlapped with the pretraining data, (2) across all datasets, we do not see a correlation that\n",
      "evaluating on clean data does worse than evaluating on the total dataset, and (3) as datasets had fewer\n",
      "clean examples, there was higher variance in the percent change in performance (likely due to a\n",
      "smaller number of clean examples).\n",
      "\n",
      "Like GPT-3, we also found that DROP and SQuADv?2 had almost total overlap with the pretraining\n",
      "data. We follow their procedure of manually inspecting the data, and find that most overlapping n-\n",
      "grams were only in the contexts of examples (99.6% for DROP and 97.2% for SQUADv2). Overlaps\n",
      "never occurred in both the question and answer for DROP, and only occurred for both the question\n",
      "and answer for SQuADv2 in 5 of the 11,153 evaluation examples. Hence, for these two datasets, the\n",
      "\n",
      "25\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "60\n",
      "\n",
      "JDROP\n",
      "40\n",
      "eval on only\n",
      "Percent change clean data\n",
      "in performance did better\n",
      "of FLAN 20\\@\n",
      "(accuracy, F1, or ANLIR1\n",
      "BLEU)\n",
      "e\n",
      "0 5 -- — - toe Wee ce\n",
      "JeSQuAD v2 RecoRD PIQA eval on all data\n",
      "e (including dirty)\n",
      "ANEIR2 did better\n",
      "209 25 50 75 100\n",
      "\n",
      "Percent of Data Clean in Dataset\n",
      "\n",
      "Figure 12: Like GPT-3, we also measured performance on cleaned versions of our datasets, which\n",
      "had high confidence to be unseen in the pretraining data of FLAN. We do not see a correlation that\n",
      "FLAN performed better on evaluation sets for which examples occurred more often in the pretraining\n",
      "data. When the percent of clean data is very small, there are fewer examples for computing the clean\n",
      "performance, which leads to high variance.\n",
      "\n",
      "model gains only background information and cannot memorize the answer to any specific questions\n",
      "(aside from the five examples in SQUADv2).\n",
      "\n",
      "ANLIRI and R2 also had almost complete data contamination, to a much higher\n",
      "degree than GPT-3. Upon further inspection, we see that most overlaps occur in example contexts\n",
      "and not hypotheses (97.3% for ANLI R1 and 98.2% for ANLI R2). As ANLI R1 and R2 are based\n",
      "entirely from Wikipedia examples (R3 is not), we posit that this higher degree of contamination in\n",
      "our pretraining dataset compared with GPT-3’s is potentially due to using a more-recent version\n",
      "of Wikipedia that includes the contexts used in ANLI R1 and R2 (which were collected in 2019).\n",
      "Because seeing a particular context in pretraining does not help with the NLI task given a new, unseen\n",
      "sentence, we think it is unlikely that these overlaps affected performance on the two datasets.\n",
      "\n",
      "Of the remaining datasets, only ReCoRD and PIQA had a clean subset performance that was lower\n",
      "\n",
      "than the overall evaluation set performance by more than 1%. These two datasets are language\n",
      "\n",
      "modeling (i.e., “what’s the best continuation of this sentence?”), and so it is more likely compared\n",
      "\n",
      "with previous tasks that seeing a complete sentence in the pretraining data could help the model\n",
      "\n",
      "predict the right answer in downstream evaluations. For PIQA, both the goal and solution had\n",
      "\n",
      "overlaps in 93 of the 1,838 evaluation examples, and for ReCoRD, the query had overlaps in 2,320 of\n",
      "\n",
      "the 10,000 training examples. We hence mark these results with an asterisk * in Table[2]\n",
      "0) also reported substantial contamination rates for these two datasets (61% dirty for ReCoRD\n",
      "\n",
      "9% for PIQA), and also mark PIQA with an asterisk.\n",
      "\n",
      "As this overlap analysis follows that performed in|Brown et al.](2020), we reiterate the same caveats:\n",
      "the conservative nature of our n-gram matching procedure likely introduces additional false positives;\n",
      "there are no guarantees that the clean subset is drawn from the same distribution as the overall subset;\n",
      "and, accurately detecting test contamination is a relatively new research area without established best\n",
      "practices. Moreover, as our pretraining corpus is almost five times larger than that used for GPT-3\n",
      "(which was 500B tokens), it is possible that there are more false positives in detecting dirty data.\n",
      "\n",
      "26\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "% Diff\n",
      ".. Total Total Clean Clean\n",
      "\n",
      "Dataset Metric count acc/FI/BLEU count acc/F1/BLEU % clean (clean —\n",
      "\n",
      "overall)\n",
      "DROP FL 9,536 22.4 61 33.0 0.6 47.4\n",
      "SQuADv2 FL 1,873 41.3 106 38.7 0.9 -6.2\n",
      "ANLIR1I acc 1,000 48.1 14 57.1 1.4 18.8\n",
      "ANLI R2 acc 1,000 42.9 21 38.1 2.1 -11.2\n",
      "ReCoRD acc 0,000 4.6 3,203 4.5 32.0 -2.7\n",
      "MultiRC acc 4,848 75.4 1,972 75.7 40.7 0.5\n",
      "PIQA acc 1,838 23.7 896 23.3 48.7 -1.7\n",
      "ANLI R3 acc 1,200 44.2 718 45.3 59.8 2.5\n",
      "HellaSwag acc 0,042 28.5 6,578 28.7 65.5 0.7\n",
      "RTE acc 2,77 84.1 183 84.2 66.1 0.0\n",
      "WMT’14En-+Fr BLEU 3,003 31.3 2,243 31.5 T4.7 0.9\n",
      "WMT’14Fr>En BLEU 3,003 34.0 2,243 34.1 T4.7 0.2\n",
      "BoolQ acc 3,270 76.5 2,515 76.3 76.9 -0.4\n",
      "TQA (tfds-dev) F 1,313 62.2 8,731 62.0 712 -0.2\n",
      "ARC Easy acc 2,365 79.5 1,888 79.0 79.8 -0.6\n",
      "ARC Challenge acc 165 63.1 983 64.2 84.4 1.7\n",
      "OpenbookQA acc 500 74.6 425 74.8 85.0 0.3\n",
      "WMT’16En—-De BLEU 2,999 22.7 2,569 23.0 85.7 1.4\n",
      "WMT’16 De-En BLEU 2,999 38.6 2,569 38.7 85.7 0.2\n",
      "WMT’16En-—-Ro BLEU 1,999 15.5 1,752 15.4 87.6 -0.7\n",
      "WMT’16 Ro-En BLEU 1,999 36.7 1,752 36.8 87.6 0.1\n",
      "COPA acc 100 88.0 91 87.9 91.0 -0.1\n",
      "CB acc 56 41.1 53 41.5 94.6 1\n",
      "NQ F 3,610 24.5 3,495 24.3 96.8 -0.5\n",
      "StoryCloze acc 3871 92.1 1,864 92.1 99.6 0.0\n",
      "Winogrande acc ;267 39.4 1,265 39.4 99.8 0.2\n",
      "\n",
      "Table 5: Overlap statistics for the subset of datasets that are also used in GPT-3, sorted from dirtiest\n",
      "to cleanest. An evaluation example was dirty if it had any n-gram collision with the pretraining\n",
      "\n",
      "corpus. We computed these results for FLAN’s performance using only a single template for each\n",
      "dataset, so they differ slightly compared with the results for average performance over all templates.\n",
      "\n",
      "27\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "D_ EXTENDED RELATED WORK\n",
      "\n",
      "D.1 LANGUAGE MODELS AND MULTI-TASK LEARNING\n",
      "\n",
      "Our work is broadly inspired by a long line of prior work on language models for NLP applications\n",
      "\n",
      "inter\n",
      "alia). Instruction tuning can be seen as a formulation of multitask learning (MTL), which is an\n",
      "established area within deep learning (Collobert et al.\n",
      "& Daniel] 2018} [Clark et al.|/2019b} |Liu et al. inter alia)—see (2020) for\n",
      "a recent survey on MTL for NLP. Differing from prior MTL work which focuses on performance\n",
      "improvements across training tasks (Raffel et al.| (2021) or to new domains\n",
      "(Axelrod et al.|/2011), our work is motivated by improving zero-shot generalization to tasks that were\n",
      "not seen in training.\n",
      "\n",
      "D.2. ZERO-SHOT LEARNING AND META-LEARNING\n",
      "\n",
      "Our work also falls in the well-established category of zero-shot learning, which has historically\n",
      "\n",
      "been used to refer to classifying instances among a set of unseen categories (Lampert et al.}/2009}\n",
      "Romera-Paredes & Torr| 2015} Srivastava et al.| 2018} Yin et al.| 2019} inter alia). In NLP, zero-shot\n",
      "\n",
      "learning work also includes translating between unseen language pairs (Johnson et al.|\n",
      "et al} 2019), language modeling on unseen languages (Lauscher et al.|/2020), as well as various NLP\n",
      "applications (Liu et al.||2019a\\|Corazza et al.||2020}|Wang et al.|/2021). Most recently, the emergent\n",
      "ability of language models onee rai) has led to increased interest in how models generalize\n",
      "\n",
      "to unseen tasks, the definition of zero-shot learning used in our paper. In addition, meta-learning\n",
      "\n",
      "(Finn et al.|/2017}[Vanschoren} [2018] inter alia) also broadly tries to train models that adapt quickly\n",
      "\n",
      "to unseen tasks, typically based on a few examples.\n",
      "\n",
      "D.3. PROMPTING\n",
      "\n",
      "Instruction tuning leverages the intuition that language models at scale contain substantial world\n",
      "\n",
      "knowledge and can perform a range of NLP tasks (Brown et al.|/2020] see also[Bommasani et al.|\n",
      "\n",
      "(2021). Another line of work that shares this goal prompts models with continuous inputs optimized\n",
      "via backpropagation to substantially improve performance (Li & Liang} |202 1} 2021]\n",
      "(2021), as well as work that prompts models to produce specialized outputs (Wei et al.|\n",
      "(2022). Although the success of these approaches depends heavily on model scale (Lester et al.|/2021),\n",
      "for which large models can be costly to serve, the ability of a single large model to perform many\n",
      "tasks slightly eases this burden. As shown by our experiments in prompt tuning is an orthogonal\n",
      "method for which instruction tuning can additionally improve performance. is\n",
      "similar to our work in that they also use related tasks to improve zero-shot learning, though they\n",
      "differ by only using related tasks in the context (and not finetuning), and focus on the application of\n",
      "text style transfer.\n",
      "\n",
      "Our work shares similar motivations with prompting in that we use inference-time text interactions\n",
      "to prompt a single model, without creating separate checkpoints for each task. Whereas prompting\n",
      "work such as GPT-3 uses prompt engineering to write prompts that intentionally mimic text that is\n",
      "likely to be seen during pretraining (e.g., for MultiRC GPT-3 tries a prompt that mimics a test with\n",
      "an answer key), we hope that finetuning models to respond to natural language instructions instead of\n",
      "completing a sentence will make such large models more accessible to non-technical users.\n",
      "\n",
      "D.4_ FINETUNING LARGE LANGUAGE MODELS\n",
      "\n",
      "Finetuning pretrained language models is a well-established method in NLP, with much of the work\n",
      "so far occurring on models in the range of 100M to 10B parameters (Dai & Le}|2015}|Devlin et al.\n",
      "2019} Raffel et al.| 2020} Lewis et al} 2020} inter alia). For models of O(100B) parameters, recent\n",
      "work has finetuned task-specific models for program synthesis 2021\n",
      "2021), summarization 2021), as well as improved bias and fairness behavior (Solaiman|\n",
      "(2021). In addition to the traditional “dense” models, sparse mixture of experts (MoE)\n",
      "\n",
      "models of up to more than 1T parameters have been trained and finetuned (Lepikhin et al.|{2020\n",
      "\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "1). Compared with this prior work that finetunes and evaluates on the same downstream\n",
      "task, our setup studies the effect of instruction tuning on ability to perform unseen tasks.\n",
      "\n",
      "D.5 MULTI-TASK QUESTION ANSWERING\n",
      "\n",
      "The instructions we use for instruction tuning are similar to QA-based task formulation research,\n",
      "which aims to unify NLP tasks by casting them as question-answering over a context. For instance,\n",
      "[McCann et al.| (2018) cast ten NLP tasks as QA and train a model on a collection of tasks formulated\n",
      "with natural language prompts; they report transfer learning gains on finetuning tasks as well as\n",
      "zero-shot domain adaptation results on SNLI (Bowman et al. and Amazon/Yelp Reviews\n",
      "\n",
      "(Kotzias et al. 2015p. While|McCann et al.|(2018) does not leverage unsupervised pre-training and\n",
      "\n",
      "only reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on\n",
      "zero-shot performance on unseen task clusters. UnifiedQA shows similar\n",
      "transfer learning gains as (2018) across 20 datasets and reports good generalization\n",
      "to unseen tasks across four types of QA. Focusing on binary text classification, [Zhong et al.]\n",
      "\n",
      "finetune T5-770M on 43 tasks phrased as yes/no questions and study the zero-shot performance on\n",
      "unseen tasks. In comparison, our paper is much larger in scope, empirically demonstrating the idea\n",
      "on a wide range of tasks with a much larger model. Other work has used QA-based task formulation\n",
      "for more-targeted applications including semantic role labeling (He et al.|[2015), relation extraction\n",
      "\n",
      "(Levy et al.||2017), coreference resolution and named entity recognition\n",
      "2020) as question answering.\n",
      "\n",
      "D.6 INSTRUCTIONS-BASED NLP\n",
      "\n",
      "Recent improvements in the capabilities of language models have led to increased interest in a nascent\n",
      "area of instructions-based NLP (Goldwasser & Roth}|2014} and see|McCarthy 1960}). Schick 4\n",
      "(also see 2021} 2021) use task descriptions in cloze-style\n",
      "\n",
      "phrases to help language models assign soft labels for few-shot and semi-supervised learning, though\n",
      "this line of work finetunes new checkpoints for each downstream task. (2020) evaluated\n",
      "GPT-2 (Radford et al.|/2019) on simple tasks ranging from retrieving the nth word of a sentence to\n",
      "\n",
      "generating examples for SQUAD, concluding that GPT-2 performs poorly across all tasks.\n",
      "\n",
      "In terms of the setup of finetuning on a large number of tasks and evaluating on unseen tasks, two\n",
      "recent papers are similar to ours. finetune BART (Lewis et al.|{2020) usi\n",
      "instructions and few-shot examples for tasks such as question answering, text classification, and text\n",
      "modification, and find that this few-shot finetuning with instructions improves performance on unseen\n",
      "tasks. introduce a setup for cross-task few-shot learning, finding that multi-task\n",
      "meta-learning using MAML improves the few-shot capabilities of BART on unseen\n",
      "downstream tasks. Our work differs from these two papers in that we focus on zero-shot learning, for\n",
      "which we observe the crucial importance of model scale (FLAN is 1,000x larger than BART-base).\n",
      "\n",
      "Perhaps the papers most related to ours are the recent/Sanh et al.|(2021) and [Min et al.|(2021), which\n",
      "were released after our initial preprint. (2021) finetunes GPT-2 Large (770M parameters)\n",
      "to be a few-shot learner, which is the same approach as our experiment in Section 4.3. Similar\n",
      "to our conclusions, they also observe that including few-shot exemplars and instruction tuning are\n",
      "complementary ways to improve performance. {Sanh et al.| (2021) propose to finetune T5-11B to\n",
      "respond to prompts, and they also report performance improvements on zero-shot learning. These\n",
      "two papers and our work all study finetuning with instructions, but, as noted by (2021),\n",
      "it is hard to directly compare results, due to differing model sizes, model types (decoder-only vs\n",
      "encoder-decoder), pretraining data, task mixtures, and type of instructions\n",
      "their instructions are more diverse).\n",
      "\n",
      "Finally, OpenAI has a model called InstructGPT (Ouyang et al.|/2022). InstructGPT uses human\n",
      "anntations to guide desired model behavior, both via finetuning and reinforcement learning, finding\n",
      "that InstructGPT is preferred by human rathers compared with unmodified GPT-3.\n",
      "\n",
      "29\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "E FREQUENTLY ASKED QUESTIONS\n",
      "\n",
      "How do the FLAN instructions differ from GPT-3 or T5 prompts?\n",
      "\n",
      "GPT-3 prompting is done in a way such that the prompt looks like data that the model has been\n",
      "pretrained on, and the model finishes the continuation. T5 prompts are mostly just a tag for the\n",
      "dataset, which would not work in the zero-shot setting. In contrast, the prompts that we use for FLAN\n",
      "are similar to what would be used to ask a human to perform the task.\n",
      "\n",
      "For instance, given an input for an NLI task, these would be the prompts.\n",
      "\n",
      "TS prompt:\n",
      "\n",
      "cb hypothesis: At my age you will probably have learnt one lesson.\n",
      "premise: It’s not certain how many lessons you’ll learn by your\n",
      "thirties.\n",
      "\n",
      "GPT-3 prompt:\n",
      "\n",
      "At my age you will probably have learnt one lesson.\n",
      "\n",
      "question: It’s not certain how many lessons you’ll learn by your\n",
      "thirties. true, false, or neither? answer:\n",
      "\n",
      "FLAN prompt:\n",
      "\n",
      "Premise: At my age you will probably have learnt one lesson.\n",
      "Hypothesis: It’s not certain how many lessons you’1l learn by your\n",
      "thirties.\n",
      "\n",
      "Does the premise entail the hypothesis?\n",
      "\n",
      "So because FLAN prompts are formulated as responding to an instruction, they do not work well\n",
      "for pretrained language models without finetuning. Performance was near zero for most generation\n",
      "tasks. For instance, given the input “‘The dog runs.’ Translate this sentence to French.” , LAMDA-PT\n",
      "continues with ”The dog runs after the cat” instead of actually translating the sentence. Hence, we\n",
      "used the established GPT-3 prompts for our LaMDA-PT baselines.\n",
      "\n",
      "What are some limitations/failure cases of FLAN?\n",
      "\n",
      "While we qualitatively find that FLAN responds well to most tasks, it does fail on some simple tasks.\n",
      "For instance, as shown in Figure[22] FLAN fails at the very simple task of returning the second word\n",
      "in a sentence, and also incorrectly translates a question to Danish when asked to answer the question\n",
      "in Danish. Additional limitations include a context length of only 1024 tokens (which is not enough\n",
      "for most summarization tasks), and that the model was mostly trained on English data.\n",
      "\n",
      "Can FLAN be used when large amounts of training data are available?\n",
      "\n",
      "In this work, we focus on cross-task generalization to zero-shot tasks, but we also believe that\n",
      "instruction tuning could result in positive task transfer among seen tasks, depending on the mixture\n",
      "of tasks (though we leave this for future work). In where we apply prompt tuning to the FLAN\n",
      "checkpoint, we see promising results that indicate positive task transfer in a supervised setting.\n",
      "\n",
      "Are the ten unique templates per dataset or per task cluster?\n",
      "\n",
      "The ten unique templates are for each dataset and not for a task cluster. This is because datasets in the\n",
      "same task cluster often differed slightly (e.g., “is this movie review positive” vs “is this yelp review\n",
      "positive”).\n",
      "\n",
      "In Figure (7A, why does the untuned LaMDA-PT model see worse performance with more\n",
      "parameters for reading comprehension and sentiment analysis?\n",
      "\n",
      "For context, Figure[7 is a check of correctness for Figure[7B. Figure[7A confirms that scale improves\n",
      "performance for tasks that were seen during instruction tuning, as expected. The untuned LaAMDA-PT\n",
      "model performance in Figure[7A is shown just for completeness.\n",
      "\n",
      "30\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Nonetheless, the fact that scale does not always improve zero-shot performance of untuned LaMDA-\n",
      "\n",
      "PT is an interesting artifact. Initially, we were surprised, because (2020) shows that scale\n",
      "improves performance across a large number of tasks in aggregate.\n",
      "\n",
      "It turns out that scale does not improve performance for certain tasks. This is especially true for\n",
      "zero-shot learning, and we think that this happens to be the case for the reading comprehension\n",
      "and sentiment analysis tasks we evaluate. The GPT-3 paper itself similarly reports that zero-shot\n",
      "performance on BoolQ and DROP decreases from 13B to 175B parameters. The GPT-3 paper does\n",
      "not show results on sentiment analysis, but{Holtzman et al.| (2021) find that zero-shot performance on\n",
      "SST-2 also gets worse from 13B to 175B parameters. Hence, this artifact is consistent across both\n",
      "GPT-3 and the models we use.\n",
      "\n",
      "This artifact is certainly worth further study, but is outside the scope of instruction tuning. Ideally,\n",
      "we would have performed the Figure[/Jablation with cross-validation instead of a single split, which\n",
      "likely would have smoothed out that artifact.\n",
      "\n",
      "F QUALITATIVE EXAMPLES\n",
      "\n",
      "This section shows qualitative examples of FLAN responding to various prompts.\n",
      "\n",
      "Input: Input:\n",
      "Movie review: This movie is the Movie review: This movie is the\n",
      "best RomCom since Pretty Woman. best RomCom since Pretty Woman.\n",
      "Did this critic like the movie? Did this critic dislike the movie?\n",
      "OPTIONS OPTIONS\n",
      "-yes -yes\n",
      "-no -no\n",
      "\n",
      "FLAN output: FLAN output:\n",
      "\n",
      "yes no\n",
      "\n",
      "Figure 13: For sentiment analysis, FLAN changes the answer appropriately when the question is\n",
      "flipped.\n",
      "\n",
      "Input: Input:\n",
      "What is Barack Obama’s What is Barack Obama’s\n",
      "profession? profession?\n",
      "Answer in Chinese\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "© politician FLAN output:\n",
      "\n",
      "¢ lawyer\n",
      "\n",
      "senator BR\n",
      "\n",
      "© us president CEES uit\n",
      "\n",
      "Figure 14: For question answering, FLAN can answer a question in another language when instructed\n",
      "to do so. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and\n",
      "top k of 40.\n",
      "\n",
      "31\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Input:\n",
      "\n",
      "John is reading the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Change to past tense.\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "John was reading the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Input:\n",
      "\n",
      "John is reading the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Change to the verb to eat.\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "John is eating the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Input:\n",
      "\n",
      "John is reading the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Change to passive voice.\n",
      "\n",
      "FLAN output:\n",
      "The paper “Finetuned Language\n",
      "\n",
      "Models are Zero-Shot Learners” is\n",
      "being read by John.\n",
      "\n",
      "Input:\n",
      "\n",
      "John is reading the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Change the verb to eat and the\n",
      "subject to Noam Chomsky.\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "Noam Chomsky is eating the paper\n",
      "“Finetuned Language Models are\n",
      "Zero-Shot Learners.”\n",
      "\n",
      "Figure 15: FLAN can perform zero-shot rule-based manipulations.\n",
      "\n",
      "Input:\n",
      "\n",
      "Recommend fun activities to do on\n",
      "a rainy night in Seattle.\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "© Go to the pacific science center\n",
      "© Go out to eat\n",
      "\n",
      "© Go to a comedy club\n",
      "\n",
      "¢ Build a blanket fort\n",
      "\n",
      "© Go to the Space Needle\n",
      "\n",
      "© Go to underground tours\n",
      "\n",
      "¢ Find a karaoke bar\n",
      "\n",
      "Input:\n",
      "\n",
      "Recommend activities to do ona\n",
      "sunny weekend in Copenhagen.\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "¢ Visit Christiania\n",
      "\n",
      "© Copenhagen Zoo\n",
      "\n",
      "¢ Visit the Tivoli Gardens\n",
      "\n",
      "¢ Take a canal tour\n",
      "\n",
      "¢ Museums\n",
      "\n",
      "© Visit The Little Mermaid Statue\n",
      "\n",
      "¢ Visit Copenhagen Zoo\n",
      "\n",
      "¢ Visit the Danish Maritime Museum\n",
      "\n",
      "Figure 16: FLAN can make zero-shot recommendations. Multiple FLAN outputs are generated via\n",
      "random sampling with a temperature of 0.9 and top k of 40.\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Input: Input:\n",
      "Generate utterances with the intent Generate utterances with the intent\n",
      "“get COVID vaccine\" “where to vote in the state Senate\n",
      "election\"\n",
      "FLAN output:\n",
      "\n",
      "FLAN output:\n",
      "\n",
      "¢ | would like to get the covid\n",
      "Tell me where to vote in the state\n",
      "\n",
      "vaccine. u\n",
      "\n",
      "© | would like a COVID vaccine as senate election :\n",
      "\n",
      "soon as possible e Find out where to vote in the state\n",
      "\n",
      "© Can | get a COVID vaccine as a senate election :\n",
      "\n",
      "preventative measure? e Where am i supposed to vote in\n",
      "\n",
      "© Get COVID vaccine for a five year the state senate election _\n",
      "\n",
      "old Where can i get information on\n",
      "\n",
      "Where is a center for COVID where to vote in the state Senate\n",
      "election?\n",
      "\n",
      "vaccine testing?\n",
      "\n",
      "Figure 17: FLAN can be used in a zero-shot fashion to generate data, such as utterances that are\n",
      "consistent with a given intent. Multiple FLAN outputs are generated via random sampling with a\n",
      "temperature of 0.9 and top k of 40.\n",
      "\n",
      "Input:\n",
      "Input: Suggest a related search term to\n",
      "Suggest a related search term to “Novak Djokovic\"\n",
      "“watermelon”\n",
      "FLAN output:\n",
      "UN ois  novak djokovic girlfriend\n",
      "© Cantaloupe serena williams\n",
      "¢ Cucumber  rafael nadal\n",
      "¢ Melon  novak djokovic wife\n",
      "@ Fruit © roger federer\n",
      "© Watermelon juice andy murray\n",
      "\n",
      "e Watermelons  novak djokovic news\n",
      "e Lemon @ novak djokovic serbia\n",
      "\n",
      "¢ Apple  novak djokovic wimbledon\n",
      "\n",
      "Figure 18: FLAN can be used for zero-shot query expansion. Multiple FLAN outputs are generated\n",
      "via random sampling with a temperature of 0.9 and top k of 40.\n",
      "\n",
      "Input:\n",
      "\n",
      "On the way to Yosemite, our front\n",
      "\n",
      "tire blew.\n",
      "\n",
      "Finish writing the story, and include Input:\n",
      "\n",
      "a happy ending. \"I guess | need to do laundry.”\n",
      "\n",
      "Rewrite the above sentence in a\n",
      "\n",
      "FLAN output (cherrypicked): Shakespeareaniatylek\n",
      "\n",
      "¢ We got our tire replaced. The\n",
      "\n",
      "mechanic was honest, kind and FLAN output:\n",
      "helpful. Then we had a smooth ride\n",
      "\n",
      "for the rest of the trip. Our trip was © To my mind, verily, | suppose 'tis\n",
      "filled with fun and excitement. time, | wash my clothes.\n",
      "\n",
      "@ We waited three hours for the Mine eyes and ears and tongue\n",
      "tow truck, but had a great picnic. declare me idle; and yet it is true |\n",
      "The tow truck driver helped us put guess, By my garments, that | need\n",
      "the spare on, and we arrived to do laundry\n",
      "\n",
      "in Yosemite for our reservations ¢ Lo my duty now calls To thee, to\n",
      "dinner, with time to spare. wash this body's clothes.\n",
      "\n",
      "Figure 19: FLAN can perform zero-shot tasks relevant to assisted-writing applications. Multiple\n",
      "FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40.\n",
      "\n",
      "33\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "Input:\n",
      "Make up a new word that means Input:\n",
      "“when bananas shower from the\n",
      "sky\" Make up a word that means \"like a\n",
      "pipedream\"\n",
      "FLAN output:\n",
      "Oo Baaeuctien FLAN output (cherrypicked):\n",
      "e Banana shower ¢ Pipedreamic\n",
      "¢ Banania e Pipedreamish\n",
      "¢ Bananafall e Pipedreamlike\n",
      "e Bananada e Phantasmagoric\n",
      "\n",
      "Figure 20: FLAN can be used for zero-shot word formation. Multiple FLAN outputs are generated\n",
      "via random sampling with a temperature of 0.9 and top k of 40.\n",
      "\n",
      "Input:\n",
      "\n",
      "Input:\n",
      "John moved to mountain view. He .\n",
      "spent friday night in San Francisco. Write a sad story about carrots.\n",
      "He got a COVID vaccine. He voted\n",
      "in the state senate election. FLAN output:\n",
      "Write a story that follows the above The sun had barely risen when the carrots were taken\n",
      "plot. from their beds and brought to market. They'd had no\n",
      "time to gather their strength. They smelled of earth,\n",
      "FLAN output (cherrypicked): their greens still wet from the dew. At first no one was\n",
      "interested in buying them. But one man finally reached\n",
      "John arrived at the city with his vaccine, out and picked up one of the carrots. \"I'll take this\n",
      "feeling prepared enough for the state one!” he cried out. \"It's for my mother! I'll pay you well\n",
      "senate election of San Francisco. He had for it.” The man took the carrot to the cashier, eager to\n",
      "\n",
      "get it home to his mother. He was careful to wrap it\n",
      "carefully so that it wouldn't get bruised or dirty. He\n",
      "hurried home, and when he saw his mother, he gave\n",
      "\n",
      "voted for the first time last night and was\n",
      "feeling good about it. He felt more\n",
      "\n",
      "prepared than ever to win, and win big, her the carrot. “Is this one of the ones | grew?\" she\n",
      "because he was getting a COVID vaccine. asked, her voice wavering with emotion. \"Yes, mother,\"\n",
      "He knew that his opponent had not he replied. “It is, dear.\" The man left the carrot with his\n",
      "\n",
      "‘ mother and hurried back to market. His mother was\n",
      "gotten a COVID vaccine, and he knew he very sick, and he was afraid she wouldn't last much\n",
      "\n",
      "had already defeated him before the longer. He needed to buy all the carrots in market so\n",
      "election began. that none would be left when she was gone.\n",
      "\n",
      "Figure 21: Open-ended generation tasks by FLAN. The carrot story was from sampling sixteen\n",
      "outputs with a minimum length of 150 and choosing the highest probability output.\n",
      "\n",
      "Input: Input:\n",
      "Conan went to the store to buy What is Barack Obama’s\n",
      "some pencils. profession?\n",
      "\n",
      "What is the second word in this Aqevertin Baatkty\n",
      "\n",
      "sentence?\n",
      "FLAN output: FLAN output:\n",
      "© pencils ¢ Hvad er Barack Obamas\n",
      "¢ Conan beskzeftigelse?\n",
      "¢ store ¢ Hvad er Barack Obamas\n",
      "¢ buy erhverv?\n",
      "\n",
      "Figure 22: Example failure cases for FLAN. Left: FLAN fails to perform a simple task of returning\n",
      "the nth word. Right: FLAN translates a question instead of answering it. Multiple FLAN outputs are\n",
      "generated via random sampling with a temperature of 0.9 and top k of 40.\n",
      "\n",
      "34\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "CHANGES FROM V4 TO V5\n",
      "\n",
      "¢ Replaced the tables in the main figure with a figure, which takes up less space and focuses\n",
      "on zero-shot performance.\n",
      "\n",
      "« Added GLaM 64B/64E as a baseline.\n",
      "\n",
      "* Moved the ablation about the role of instructions, as well as prompt tuning, into the main\n",
      "paper (and condensed the figures).\n",
      "CHANGES TO V4 FROM V3\n",
      "¢ We added a Frequently Asked Questions section (Appendix |E}.\n",
      "\n",
      "¢ We added a section with qualitative examples (Appendi.\n",
      "\n",
      "¢ We added an additional ablation study on the role of instructions during finetuning (Ap-\n",
      "pendix|[B.2).\n",
      "\n",
      "¢ We updated the related work (Appendix [D} with manuscripts posted on arxiv since our\n",
      "initial preprint.\n",
      "\n",
      "CHANGES TO V3 FROM V2\n",
      "\n",
      "¢ The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.\n",
      "\n",
      "CHANGES TO V2 FROM V1\n",
      "\n",
      "¢ We updated the terminology to “datasets” and “task clusters.”\n",
      "¢ We renamed the previous “open-domain QA” task cluster to “closed-book QA.”\n",
      "\n",
      "¢ We extended the related work section and moved it to the Appendix [D] using a shorter\n",
      "version in the main body.\n",
      "\n",
      "¢ We added FLAN and LaMDA-PT results for additional datasets for which GPT-3 results\n",
      "were not reported.\n",
      "\n",
      "¢ For TriviaQA, v1 reported results on the tfds dev set of 11,313 examples. GPT-3 actually\n",
      "evaluates on the wikipedia dev set of 7,993 examples, so we ran an additional evaluation\n",
      "on that dev set in order to compare with GPT-3’s performance. Zero-shot FLAN now beats\n",
      "zero-shot GPT-3 on that task (and therefore on 20 of 25 tasks). We still show the original\n",
      "result in Table[2| though there is no GPT-3 result to compare with.\n",
      "\n",
      "¢ We moved commonsense reasoning and coreference resolution from the main body to the\n",
      "Appendix.\n",
      "\n",
      "¢ We moved prompt tuning from the main body to\n",
      "\n",
      "¢ We added data contamination analysis (Appendix|C}.\n",
      "\n",
      "¢ We added few-shot instruction tuning ($4.4).\n",
      "\n",
      "¢ We cited additional datasets in Appendix[G]\n",
      "\n",
      "¢ The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.\n",
      "\n",
      "35\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G_ TASKS AND DATASETS\n",
      "\n",
      "This appendix further details the datasets that we use in this paper. We group datasets into one of the\n",
      "following task clusters:\n",
      "\n",
      "¢ Natural language inference concerns how two sentences relate, typically asking, given a first\n",
      "sentence, whether a second sentence is true, false, or possibly true. We use the following datasets:\n",
      "\n",
      "1. ANLI\n",
      "\n",
      "CB (De Manele etal, 2019\n",
      "\n",
      "MNLI\n",
      "\n",
      "ONLI\n",
      "\n",
      "SNLI\n",
      "\n",
      "WNLI\n",
      "\n",
      "7. RTE (Dagan et al.|[2005} [Haim et al.|/2006} |Giampiccolo et al.||2007} [Bentivogli et al.|{2009)\n",
      "\n",
      "¢ Reading comprehension tests the ability to answer a question when given a passage that contains\n",
      "the answer. We use the following datasets:\n",
      "\n",
      "1. BoolQ[Clark eta (20103\n",
      "DROP\n",
      "MuliRC\n",
      "OBQA (Mihaylov etal 20T8)\n",
      "SQuADWI (Rajpurkar etal 2076)\n",
      "6. SQuADv2 (Rajpurkar et al|\n",
      "\n",
      "¢ Commonsense reasoning evaluates the ability to perform physical or scientific reasoning with an\n",
      "element of common sense. We use the following datasets:\n",
      "\n",
      "1. COPA\n",
      "2. HellaSwag (Zellers et al.|{2019)\n",
      "\n",
      "3. PIQA (BIsk eal [2020)\n",
      "\n",
      "4. StoryCloze\n",
      "\n",
      "¢ Sentiment analysis is a classic NLP task aims to understand whether a piece of text is positive or\n",
      "negative. We use the following datasets:\n",
      "\n",
      "|. IMDB\n",
      "2. Sentiment140\n",
      "3, SSP2\n",
      "\n",
      "4. Yelp (Fast.Al)\n",
      "\n",
      "* Closed-book QA asks models to answer questions about the world without specific access to\n",
      "information that contains the answer. We use the following datasets:\n",
      "\n",
      "1. ARC (Clark etal] BOT8)\n",
      "2. NQ (Lee et al. 2019} Kwiatkowski et al. 2019)\n",
      "3. TriviaQA Joshi et al.}(2017)\n",
      "\n",
      "¢ Paraphrase detection asks a model to determine whether two sentences are semantically equiva-\n",
      "lent{\"| We use the following datasets:\n",
      "\n",
      "1. MRPC (Dolan & Brockett} |2005)\n",
      "2. QOP see)\n",
      "3. Paws Wiki Zhang etal 2019)\n",
      "\n",
      "* Coreference resolution tests the ability to identify expressions of the same entity in some given\n",
      "text. We use the following datasets:\n",
      "\n",
      "1. DPR (Rahman & Ng| 2072)\n",
      "2. Winogrande (Sakaguchi et al.|/2020)\n",
      "\n",
      "4 Although paraphrasing can be seen as positive entailment in both directions, it has been distinct from NLI\n",
      "in the academic literature.\n",
      "\n",
      "NAwPFwnN\n",
      "\n",
      "ARYwWN\n",
      "\n",
      "36\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "3. WSC273 (Levesque et al.|2012)\n",
      "\n",
      "Reading comprehension with commonsense combines elements of both reading comprehension\n",
      "with commonsense. We use the following datasets:\n",
      "\n",
      "1. CosmosQA (Huang et al.|/2019)\n",
      "2. ReCoRD (Zhang et al.|/2018)\n",
      "\n",
      "Struct to text tests the ability to describe some structured data using natural language. We use the\n",
      "following datasets:\n",
      "\n",
      "1. CommonGen (Lin et al.}[2020)\n",
      "2. DART (Nan et al202)\n",
      "\n",
      "3. EZENLG\n",
      "4. WebNLG\n",
      "\n",
      "Translation is the task of translating text from one language into a different language. We use the\n",
      "following datasets:\n",
      "\n",
      "1. En—Fr from WMT’ 14 (Bojar et al.|/2014)\n",
      "2. En—De, En-Tr, En—Cs, En—Fi, En—Ro, and En—Ru from WMT’ 16 (Bojar et al.|/2016)\n",
      "3. En-Es from Paracrawl 2020)\n",
      "\n",
      "¢ Summarization asks models to read a piece of text and generate an abbreviated summary of it.\n",
      "We use the following datasets:\n",
      "\n",
      "1. ABSLC (Zhang & Tee 2019)\n",
      "\n",
      "CNN-DM\n",
      "\n",
      "Gigaword (Napotes et a 2OT2)\n",
      "\n",
      "MultiNews (Fabbri et al.|[2019)\n",
      "\n",
      "Newsroom (Grusky et al.|/2018)\n",
      "\n",
      "Samsuim (iva 2019\n",
      "\n",
      "xSum\n",
      "\n",
      "AG News\n",
      "\n",
      "Opinion Abstracts - Rotten Tomatoes\n",
      "Opinion Abstracts - iDebate\n",
      "11. Wiki Lingua English\n",
      "\n",
      "¢ Additional datasets that we assign to a miscellaneous task cluster include:\n",
      "\n",
      "1. Conversational question-answering: QuAC (Choi etal. 2018) and CoQA (Reddy et al. 2019)\n",
      "2. Evaluating context-sentence word meanings: WiC (Pilehvar & Camacho-Collados} 2019)\n",
      "\n",
      "3. Question classification: TREC di & Roth 2002} Hovy et al.| 2001)\n",
      "4\n",
      "5\n",
      "\n",
      "CRPNNAMAPWN\n",
      "\n",
      "_\n",
      "S\n",
      "\n",
      ". Linguistic acceptability: CoLA (Warstadt et al.|/2019)\n",
      ". Math questions 2019)\n",
      "\n",
      "For all tasks, our finetuning and evaluation code uses tensorflow datasets (TFDS) to load and process\n",
      "datasets. Regarding the number of training examples per dataset, we limited the training set size\n",
      "per dataset to 30,000 so that no dataset dominated the finetuning distribution. When a test set with\n",
      "labels was available in TFDS, we used it; otherwise, we used the TFDS validation set as our test set,\n",
      "splitting the training set into a train and dev set.\n",
      "\n",
      "On the following pages, we show inputs and outputs for evaluation tasks where we compared with\n",
      "GPT-3. See the attached supplementary material for the templates for all other datasets.\n",
      "\n",
      "37\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G.1 NATURAL LANGUAGE INFERENCE\n",
      "\n",
      "INPUT\n",
      "\n",
      "Joey Heindle (born 14 May 1993 in Munich) is a German singer. He is best known for winning\n",
      "the seventh season of the game show Ich bin ein Star — Holt mich hier raus! and finishing in 5th\n",
      "place in season 9 of Deutschland sucht den Superstar, despite universally negative reviews from\n",
      "the jury each week.\n",
      "\n",
      "Based on the paragraph above can we conclude that \"Joey Heindle was highly disliked by people\n",
      "on television.\"?\n",
      "\n",
      "OPTIONS:\n",
      "\n",
      "- Yes\n",
      "\n",
      "- It’s impossible to say\n",
      "-No\n",
      "\n",
      "TARGET\n",
      "Yes\n",
      "\n",
      "Table 6: Example input and target for Adversarial NLI (ANLI). ANLI (Nie et al.| |2020) is\n",
      "large-scale NLI benchmark with adversarial examples collected iteratively with a human and model\n",
      "in the loop. The task is to determine whether a hypothesis is entailed by a premise (entailment, not\n",
      "entailment, or impossible to say). There are three rounds, R1-R3. Of the three training sets with\n",
      "16,946, 45,460, and 100,459 examples, we use 16,946, 30,000, and 30,000 for train and 200 from\n",
      "each of the three TFDS validation sets for dev. We use the TFDS “test” sets of 1,000, 1,000, and\n",
      "1,200 examples as our test set for reporting numbers.\n",
      "\n",
      "2\n",
      "\n",
      "INPUT\n",
      "\n",
      "A: so I watch the fish, you know. Whatever I can do to keep myself occupied. I like to have the\n",
      "TV on, because that usually keeps me, um, more occupied. It kind of takes the time away and\n",
      "I don’t realize, that’s really the only time I ever watch TV, is when I’m on the bike. and then\n",
      "usually after I’m done riding the bike, just to cool myself down, I usually take a walk, you know,\n",
      "and that just kind of uh, gets me, you know, to where I’m not quite as tired I guess. But it’s\n",
      "definitely a task. B: You think so? A: I can’t say that I really enjoy it.\n",
      "\n",
      "Based on the paragraph above can we conclude that \"she really enjoys it\"?\n",
      "\n",
      "OPTIONS:\n",
      "\n",
      "- Yes\n",
      "\n",
      "-No\n",
      "\n",
      "- It’s impossible to say\n",
      "\n",
      "TARGET\n",
      "No\n",
      "\n",
      "Table 7: Example input and target for Commitment Bank (CB). CB (De Marneffe et al.|/2019) is a\n",
      "corpus of texts in which a hypothesis is extracted from a premise, and the task is to determine whether\n",
      "\n",
      "the hypothesis is entailed by the premise (entailment, not entailment, or impossible to say). Of the\n",
      "training set with 250 examples, we use 200 for train and 50 for dev. We use the TFDS validation set\n",
      "of 56 examples as our test set for reporting numbers.\n",
      "\n",
      "38\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "INPUT\n",
      "\n",
      "After years of study, the Vatican’s doctrinal congregation has sent church leaders a confidential\n",
      "document concluding that \"sex-change\" procedures do not change a person’s gender in the eyes\n",
      "of the church.\n",
      "\n",
      "Based on the paragraph above can we conclude that \"Sex-change operations become more\n",
      "common.\"?\n",
      "\n",
      "OPTIONS:\n",
      "- yes\n",
      "- no\n",
      "\n",
      "TARGET\n",
      "no\n",
      "\n",
      "Table 8: Example input and target for Recognizing Textual Entailment (RTE). RTE\n",
      "2005} Haim et al.|/2006}/Giampiccolo et al.| {2007} |Bentivogli et al.|/2009) asks whether a second\n",
      "sentence is entailed by a first (binary, either entailed or not entailed). Of the training set with 2490\n",
      "examples, we use 2,290 for train and 200 for dev. We use the TFDS validation set of 277 examples as\n",
      "our test set for reporting numbers.\n",
      "\n",
      "39\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G.2. READING COMPREHENSION\n",
      "\n",
      "INPUT\n",
      "\n",
      "There are four ways an individual can acquire Canadian citizenship: by birth on Canadian\n",
      "soil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption.\n",
      "Among them, only citizenship by birth is granted automatically with limited exceptions, while\n",
      "citizenship by descent or adoption is acquired automatically if the specified conditions have been\n",
      "met. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration,\n",
      "Refugees and Citizenship.\n",
      "\n",
      "Can we conclude that can i get canadian citizenship if my grandfather was canadian?\n",
      "OPTIONS:\n",
      "\n",
      "- no\n",
      "- yes\n",
      "\n",
      "TARGET\n",
      "no\n",
      "\n",
      "Table 9: Example input and target for Boolean Questions (BoolQ). BoolQ|Clark et al.}(2019a) asks a\n",
      "t\n",
      "\n",
      "yes/no question based on a passage and a question. Of the training set with 9,427 examples, we use\n",
      "9,227 for train and 200 for dev. We use the TFDS validation set of 3,270 examples as our test set for\n",
      "reporting numbers.\n",
      "\n",
      "INPUT\n",
      "\n",
      "Imagine you are standing in a farm field in central Illinois. The land is so flat you can see for\n",
      "miles and miles. On a clear day, you might see a grain silo 20 miles away. You might think to\n",
      "yourself, it sure is flat around here. If you drive one hundred miles to the south, the landscape\n",
      "changes. In southern Illinois, there are rolling hills. Why do you think this is? What could have\n",
      "caused these features? There are no big rivers that may have eroded and deposited this material.\n",
      "The ground is capable of supporting grass and trees, so wind erosion would not explain it. To\n",
      "answer the question, you need to go back 12,000 years. Around 12,000 years ago, a giant ice\n",
      "sheet covered much of the Midwest United States. Springfield, Illinois, was covered by over a\n",
      "mile of ice. Its hard to imagine a mile thick sheet of ice. The massive ice sheet, called a glacier,\n",
      "caused the features on the land you see today. Where did glaciers go? Where can you see them\n",
      "today? Glaciers are masses of flowing ice.\n",
      "\n",
      "Question: \"How big were the glaciers?\"\n",
      "Response: \"One mile\"\n",
      "\n",
      "Does the response correctly answer the question?\n",
      "OPTIONS:\n",
      "\n",
      "- no\n",
      "- yes\n",
      "\n",
      "TARGET\n",
      "yes\n",
      "\n",
      "Table 10: Example input and target for Multi-Sentence Reading Comprehension (MultiRC). MultiRC\n",
      "Khashabi et al.|(2018) asks an open-ended question given a paragraph that contains the answer. Of\n",
      "\n",
      "the training set with 27,243 examples, we use 27,043 for train and 200 for dev. We use the TFDS\n",
      "validation set of 4,848 examples as our test set for reporting numbers.\n",
      "\n",
      "40\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "INPUT\n",
      "soil is a renewable resource for growing plants\n",
      "A plant that needs to expand will be able to have an endless resource in\n",
      "\n",
      "OPTIONS:\n",
      "- dirt\n",
      "\n",
      "- pesticides\n",
      "~ pay\n",
      "\n",
      "- beans\n",
      "\n",
      "TARGET\n",
      "dirt\n",
      "\n",
      "Table 11: Example input and target for Openbook Question Answering (OBQA). OBQA\n",
      "asks 4-way multiple choice questions based facts. Of the training set with 4,957 examples,\n",
      "we use all for train and 200 in the TFDS validation set of 500 examples for dev. We use the TFDS\n",
      "test set of 500 examples as our test set for reporting numbers.\n",
      "\n",
      "41\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G.3. COMMONSENSE REASONING\n",
      "\n",
      "INPUT\n",
      "I packed up my belongings. What is the cause?\n",
      "\n",
      "OPTIONS:\n",
      "- I was hunting for a new apartment.\n",
      "- I was moving out of my apartment.\n",
      "\n",
      "TARGET\n",
      "I was moving out of my apartment.\n",
      "\n",
      "Table 12: Example input and target for Choice of Plausible Alternatives (COPA). COPA\n",
      "(2011) is a causal reasoning task that asks to infer either a cause of effect of a premise from\n",
      "two choices. Of the training set with 400 examples, we use 350 for train and 50 for dev. We use the\n",
      "TEDS validation set of 100 examples as our test set for reporting numbers.\n",
      "\n",
      "INPUT\n",
      "What happens next in this paragraph?\n",
      "\n",
      "Once the rope is inside the hook, he begins moving up the wall but shortly after he stops and\n",
      "begins talking. The male then begins talking about the clip again and goes back up the wall. as\n",
      "he\n",
      "\n",
      "OPTIONS:\n",
      "\n",
      "- progresses, there are hooks everywhere on the wall and when he gets near them, he puts his\n",
      "rope inside of it for support and safety.\n",
      "\n",
      "- changes time, an instant replay of his initial move is shown a second time.\n",
      "\n",
      "- continues to talk, another male speaks about the move and shows another closeup of the plex\n",
      "by the male.\n",
      "\n",
      "- continues, other people start to arrive and begin to hang out with him as he makes a few parts\n",
      "of the rope.\n",
      "\n",
      "TARGET\n",
      "progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope\n",
      "inside of it for support and safety.\n",
      "\n",
      "Table 13: Example input and target for Commonsense Sentence Completion (HellaSwag). HellaSwag\n",
      "(2019) tests for sentence completion that requires common sense, asking for the most\n",
      "probable ending given four contexts. Of the training set with 39,905 examples, we use 30,000 for\n",
      "train and 200 for dev. We use the TFDS validation set of 10,042 examples as our test set for reporting\n",
      "numbers.\n",
      "\n",
      "42\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "INPUT\n",
      "Here is a goal: Remove smell from garbage disposal.\n",
      "\n",
      "How would you accomplish this goal?\n",
      "OPTIONS:\n",
      "\n",
      "- Create soda ice cubes and grind through disposal.\n",
      "- Create vinegar ice cubes and grind through disposal.\n",
      "\n",
      "TARGET\n",
      "Create vinegar ice cubes and grind through disposal.\n",
      "\n",
      "Table 14: Example input and target for Physical Question Answering (PiQA). PiQA (Bisk et al.|\n",
      "is a commonsense QA benchmark for naive physics reasoning, where a solution to a goal must\n",
      "\n",
      "be selected from two choices. Of the training set with 16,113 examples, we use 16,013 for train and\n",
      "100 for dev. We use the TFDS validation set of 1,838 examples as our test set for reporting numbers.\n",
      "\n",
      "INPUT\n",
      "Caroline never drinks carbonated beverages. Her friends pick on her because of it. One day they\n",
      "challenged her to drink a soda. Caroline wanted to win the challenge.\n",
      "\n",
      "Predict the next sentence.\n",
      "\n",
      "OPTIONS:\n",
      "\n",
      "- Caroline refused to open the soda.\n",
      "\n",
      "- Caroline opened the soda and drank it all in one gulp!\n",
      "\n",
      "TARGET\n",
      "Caroline opened the soda and drank it all in one gulp!\n",
      "\n",
      "Table 15: Example input and target for The Story Cloze Test (StoryCloze). StoryCloze\n",
      "is a commonsense reasoning framework for story generation, where a system chooses\n",
      "the correct ending to a four-sentence story. We use the 2016 version on TFDS. Of the validation set\n",
      "with 1,871 examples (no training set is available), we use 1,671 for train and 200 for dev. We use the\n",
      "TEDS test set of 1,871 examples as our test set for reporting numbers.\n",
      "\n",
      "43\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G.4 CLOSED-BOOK QA\n",
      "\n",
      "INPUT\n",
      "What season is the Northern Hemisphere experiencing when it is tilted directly toward the Sun?\n",
      "\n",
      "OPTIONS:\n",
      "- fall\n",
      "\n",
      "- winter\n",
      "\n",
      "- spring\n",
      "\n",
      "- summer\n",
      "\n",
      "TARGET\n",
      "summer\n",
      "\n",
      "Table 16: Example input and target for The AI2 Reasoning Challenge (ARC). ARC\n",
      "asks grade-school level 4-way multiple choice science questions. There is a challenge set and\n",
      "an easy set, where the challenge set questions were answered incorrectly by both a retrieval-based\n",
      "algorithm and a co-occurrence algorithm. Of the training sets with 1,119 examples (challenge) and\n",
      "2,251 (easy), we use we use 919 and 2,051 respectively for train and 200 each for dev. We use the\n",
      "TEDS test sets of 1,172 and 2,376 examples respectively as our test set for reporting numbers.\n",
      "\n",
      "INPUT\n",
      "Question: who is the girl in more than you know??\n",
      "Answer:\n",
      "\n",
      "TARGET\n",
      "Romi Van Renterghem.\n",
      "\n",
      "Table 17: Example input and target for Natural Questions (Open) (NQ). NQ 2019\n",
      "[Kwiatkowski et al.|{2019) asks for an open-ended answer given a question, where all questions can be\n",
      "answered using the contents of Wikipedia. Of the training set of 87,925 examples, we use 30,000 for\n",
      "train and 200 for dev. We use the TFDS validation set of 3,610 examples as our test set for reporting\n",
      "numbers.\n",
      "\n",
      "INPUT\n",
      "Please answer this question: Henry Croft, an orphan street sweeper who collected money for\n",
      "\n",
      "charity, is associated with what organised charitable tradition of working class culture in London,\n",
      "England?\n",
      "\n",
      "TARGET\n",
      "pearly kings and queens\n",
      "\n",
      "Table 18: Example input and target for Trivia Question Answering (TriviaQA). TriviaQA [Joshi et al.|\n",
      "includes question-answer pairs authored by trivia enthusiasts. Of the training set of 87,622\n",
      "\n",
      "examples, we use 30,000 for train and 200 for dev. We use 7,993 examples from Wikipedia of the\n",
      "\n",
      "11,313 examples in the TFDS validation set, which is the same validation set used in\n",
      "\n",
      "{2020}. as our test set for reporting numbers.\n",
      "\n",
      "44\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G.5 COREFERENCE RESOLUTION\n",
      "\n",
      "INPUT\n",
      "How does the sentence end?\n",
      "\n",
      "Elena wanted to move out of her parents fast but Victoria wanted to stay for a while,\n",
      "OPTIONS:\n",
      "\n",
      "- Elena went to school.\n",
      "- Victoria went to school.\n",
      "\n",
      "TARGET\n",
      "Victoria went to school.\n",
      "\n",
      "Table 19: Example input and target for Adversarial Winograd Schema Challenge (Winogrande).\n",
      "Winogrande (Sakaguchi et al.|2020) tests for coreference resolution by asking a model to fill in a\n",
      "masked token in a sentence by choosing an entity from two options. Of the 40.4k examples in the XL\n",
      "training set, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 1,267 as our\n",
      "test set for reporting numbers.\n",
      "\n",
      "INPUT\n",
      "\n",
      "Jane knocked on Susan’s door, but there was no answer.\n",
      "OPTIONS:\n",
      "\n",
      "- Jane was out.\n",
      "\n",
      "- Susan was out.\n",
      "\n",
      "TARGET\n",
      "Susan was out.\n",
      "\n",
      "Table 20: Example input and target for Winograd Schema Challenge (WSC273). WSC273\n",
      "fet al.|/2012) tests for coreference resolution by asking a model to complete the sentence in a fashion\n",
      "that requires understanding the entities in the sentence. Of the 0 examples in the training set (WSC273\n",
      "is test-set only), we use none for train and none for dev. We use the TFDS test set as our test set for\n",
      "reporting numbers.\n",
      "\n",
      "45\n",
      "Published as a conference paper at ICLR 2022\n",
      "\n",
      "G.6 READING COMPREHENSION WITH COMMONSENSE\n",
      "\n",
      "INPUT\n",
      "Complete the passage.\n",
      "\n",
      "(CNN) - At first glance, \"The Flat\" might seem like an episode of \"Hoarders,\" Israeli-style. The\n",
      "documentary film opens after an elderly woman dies in Tel Aviv. Her grandchildren assemble to\n",
      "clean out her apartment, packed with dusty books, vintage clothing (dozens of pairs of fancy\n",
      "gloves, for instance), enough purses to stock a department store, jewelry, mementoes and closets\n",
      "full of knickknacks. But buried among the detritus they chance upon something remarkable —\n",
      "mysterious papers linking the grandparents to an important Nazi figure. How could such ardent\n",
      "Zionists, who left their native Germany in the early 1930s, have been involved with an SS official\n",
      "like Leopold von Mildenstein?\n",
      "\n",
      "What I found out was this journey, the Nazi (\n",
      "\n",
      "OPTIONS:\n",
      "\n",
      "- Arnon Goldfinger) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "- CNN) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- Germany) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- Israeli) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger\n",
      "told CNN.\n",
      "\n",
      "- Nazi) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- SS) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- Tel Aviv) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- The Flat) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "- Zionists) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\n",
      "\n",
      "TARGET\n",
      "Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger told\n",
      "CNN.\n",
      "\n",
      "Table 21: Example input and target for Reading Comprehension with Commonsense Reasoning\n",
      "\n",
      "(ReCoRD). ReCoRD (Zhang et al.||2018) asks for the answer to a cloze-style question where an\n",
      "entity is masked out. Of the the training set of 100,730 examples, we use 30,000 for train and 200 for\n",
      "\n",
      "dev. We use the TFDS validation set of 10,000 examples as our test set for reporting numbers.\n",
      "\n",
      "G.7 TRANSLATION (7 LANGUAGES)\n",
      "\n",
      "INPUT\n",
      "Here the largest town of the district is located: Nordenham , lying opposite to Bremerhaven at\n",
      "the Weser mouth.\n",
      "\n",
      "Translate to German\n",
      "\n",
      "TARGET\n",
      "An der B 211 befindet sich in Loyermoor der so genannte “Geest-Abbruch”, der eine Hohendif-\n",
      "ferenz von gut 30 Meter iiberbriickt.\n",
      "\n",
      "Table 22: Example input and output for translation. This example is from WMT’ 16 English-German;\n",
      "all languages use the same translation templates.\n",
      "\n",
      "46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(extract_text_with_ocr('research_paper/FLAN.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d85c3a0e",
   "metadata": {},
   "source": [
    "# Dumping the outputs into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04a93f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('output.json', 'w') as f:\n",
    "    json.dump(final_chunk_list, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
